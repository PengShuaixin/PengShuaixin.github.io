<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spark中的三种分布式部署方式对比（粗粒度模式、细粒度模式）]]></title>
    <url>%2Fpost%2FSpark%E4%B8%AD%E7%9A%84%E4%B8%89%E7%A7%8D%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F%E5%AF%B9%E6%AF%94%EF%BC%88%E7%B2%97%E7%B2%92%E5%BA%A6%E6%A8%A1%E5%BC%8F%E3%80%81%E7%BB%86%E7%B2%92%E5%BA%A6%E6%A8%A1%E5%BC%8F%EF%BC%89%2F</url>
    <content type="text"><![CDATA[本文来自董的博客,原文地址: http://dongxicheng.org/framework-on-yarn/apache-spark-comparing-three-deploying-ways/ 目前Apache Spark支持三种分布式部署方式，分别是standalone、spark on mesos和 spark on YARN其中，第一种类似于MapReduce 1.0所采用的模式，内部实现了容错性和资源管理，后两种则是未来发展的趋势，部分容错性和资源管理交由统一的资源管理系统完成：让Spark运行在一个通用的资源管理系统之上，这样可以与其他计算框架，比如MapReduce，公用一个集群资源，最大的好处是降低运维成本和提高资源利用率（资源按需分配）。本文将介绍这三种部署方式，并比较其优缺点。 standalone模式即独立模式，自带完整的服务，可单独部署到一个集群中，无需依赖任何其他资源管理系统。从一定程度上说，该模式是其他两种的基础。借鉴Spark开发模式，我们可以得到一种开发新型计算框架的一般思路：先设计出它的standalone模式，为了快速开发，起初不需要考虑服务（比如:master/slave）的容错性，之后再开发相应的wrapper，将standalone模式下的服务原封不动的部署到资源管理系统yarn或者mesos上，由资源管理系统负责服务本身的容错。目前Spark在standalone模式下是没有任何单点故障问题的，这是借助zookeeper实现的，思想类似于Hbase master单点故障解决方案。将Spark standalone与MapReduce比较，会发现它们两个在架构上是完全一致的： 1) 都是由master/slaves服务组成的，且起初master均存在单点故障，后来均通过zookeeper解决（Apache MRv1的JobTracker仍存在单点问题，但CDH版本得到了解决）； 2) 各个节点上的资源被抽象成粗粒度的slot，有多少slot就能同时运行多少task。不同的是，MapReduce将slot分为map slot和reduce slot，它们分别只能供Map Task和Reduce Task使用，而不能共享，这是MapReduce资源利率低效的原因之一，而Spark则更优化一些，它不区分slot类型，只有一种slot，可以供各种类型的Task使用，这种方式可以提高资源利用率，但是不够灵活，不能为不同类型的Task定制slot资源。总之，这两种方式各有优缺点。 Spark On Mesos模式这是很多公司采用的模式，官方推荐这种模式（当然，原因之一是血缘关系）。正是由于Spark开发之初就考虑到支持Mesos，因此，目前而言，Spark运行在Mesos上会比运行在YARN上更加灵活，更加自然。目前在Spark On Mesos环境中，用户可选择两种调度模式之一运行自己的应用程序（可参考Andrew Xia的“Mesos Scheduling Mode on Spark”）： 1) 粗粒度模式（Coarse-grained Mode）：每个应用程序的运行环境由一个Dirver和若干个Executor组成，其中，每个Executor占用若干资源，内部可运行多个Task（对应多少个“slot”）。应用程序的各个任务正式运行之前，需要将运行环境中的资源全部申请好，且运行过程中要一直占用这些资源，即使不用，最后程序运行结束后，回收这些资源。举个例子，比如你提交应用程序时，指定使用5个executor运行你的应用程序，每个executor占用5GB内存和5个CPU，每个executor内部设置了5个slot，则Mesos需要先为executor分配资源并启动它们，之后开始调度任务。另外，在程序运行过程中，mesos的master和slave并不知道executor内部各个task的运行情况，executor直接将任务状态通过内部的通信机制汇报给Driver，从一定程度上可以认为，每个应用程序利用mesos搭建了一个虚拟集群自己使用。 2) 细粒度模式（Fine-grained Mode）：鉴于粗粒度模式会造成大量资源浪费，Spark On Mesos还提供了另外一种调度模式：细粒度模式，这种模式类似于现在的云计算，思想是按需分配。与粗粒度模式一样，应用程序启动时，先会启动executor，但每个executor占用资源仅仅是自己运行所需的资源，不需要考虑将来要运行的任务，之后，mesos会为每个executor动态分配资源，每分配一些，便可以运行一个新任务，单个Task运行完之后可以马上释放对应的资源。每个Task会汇报状态给Mesos slave和Mesos Master，便于更加细粒度管理和容错，这种调度模式类似于MapReduce调度模式，每个Task完全独立，优点是便于资源控制和隔离，但缺点也很明显，短作业运行延迟大。 Spark On YARN模式这是一种最有前景的部署模式。但限于YARN自身的发展，目前仅支持粗粒度模式（Coarse-grained Mode）。这是由于YARN上的Container资源是不可以动态伸缩的，一旦Container启动之后，可使用的资源不能再发生变化，不过这个已经在YARN计划（具体参考：https://issues.apache.org/jira/browse/YARN-1197）中了。 总之，这三种分布式部署方式各有利弊，通常需要根据公司情况决定采用哪种方案。进行方案选择时，往往要考虑公司的技术路线（采用Hadoop生态系统还是其他生态系统）、服务器资源（资源有限的话就不要考虑standalone模式了）、相关技术人才储备等。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>粗粒度</tag>
        <tag>细粒度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IntelliJ IDEA 2018.3常用配置图解]]></title>
    <url>%2Fpost%2FIntelliJ-IDEA-2018-3%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE%E5%9B%BE%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[软件版本IntelliJ IDEA 2018.3 简介IDEA的优势（相对Eclipse）1.强大的整合能力。比如：Git、Maven、Spring等2.提示功能的快速、便捷3.提示功能的范围广4.好用的快捷键和代码模板5.精准搜索 安装目录结构 bin容器，执行文件和启动参数等 help快捷键文档和其他帮助文档 jre6464 位java 运行环境 libidea 依赖的类库 license各个插件许可 plugin插件设置目录结构当你不小心把IntelliJ IDEA 改坏了，删掉该目录，重启软件之后会重新生成默认配置的软件。 常用配置首先进入设置页面，快捷键：Ctrl+Alt+S目录结构如图： Appearance &amp; Behavior设置主题、字体编辑区主题的设置以及通过插件更换主题的方法在这里就不作说明的，感兴趣的可以自己去研究一下。 Editor - General设置鼠标滚轮修改字体大小 设置鼠标悬浮提示上图所示:Show quick documentation on mouse move,勾选该选项 设置自动导包功能Add unambiguous imports on the fly：自动导入不明确的结构Optimize imports on the fly：自动帮我们优化导入的包 忽略大小写提示 IntelliJ IDEA 的代码提示和补充功能有一个特性：区分大小写。如上图标注所示，默认就是 First letter 区分大小写的。 区分大小写的情况是这样的：比如我们在 Java 代码文件中输入 stringBuffer， IntelliJ IDEA 默认是不会帮我们提示或是代码补充的，但是如果我们输入StringBuffer 就可以进行代码提示和补充。 如果想不区分大小写的话，去掉勾选即可 设置取消单行显示tabs 的操作在打开很多文件的时候，IntelliJ IDEA 默认是把所有打开的文件名 Tab 单行显示的。去掉如图勾选框即可取消单行显示: 设置默认的字体、字体大小、字体行间距 Editor - File and Code Templates修改类头的文档注释信息 常用的预设的变量:12345678910111213$&#123;PACKAGE_NAME&#125; - the name of the target package where the new class or interface will be created. $&#123;PROJECT_NAME&#125; - the name of the current project. $&#123;FILE_NAME&#125; - the name of the PHP file that will be created. $&#123;NAME&#125; - the name of the new file which you specify in the New File dialog box during the file creation. $&#123;USER&#125; - the login name of the current user. $&#123;DATE&#125; - the current system date. $&#123;TIME&#125; - the current system time. $&#123;YEAR&#125; - the current year. $&#123;MONTH&#125; - the current month. $&#123;DAY&#125; - the current day of the month. $&#123;HOUR&#125; - the current hour. $&#123;MINUTE&#125; - the current minute. $&#123;PRODUCT_NAME&#125; - the name of the IDE in which the file will be created. $&#123;MONTH_NAME_SHORT&#125; - the first 3 letters of the month name. Example: Jan, Feb, etc. $&#123;MONTH_NAME_FULL&#125; - full name of a month. Example: January, February, etc. Editor – File Encodings设置项目文件编码Transparent native-to-ascii conversion 主要用于转换 ascii，一般都要勾选，不然 Properties 文件中的注释显示的都不会是中文。 设置当前源文件的编码对单独文件的编码修改还可以点击右下角的编码设置区。如果代码内容中包含中文，则会弹出如上的操作选择。其中：①Reload 表示使用新编码重新加载，新编码不会保存到文件中，重新打开此文件，旧编码是什么依旧还是什么。②Convert 表示使用新编码进行转换，新编码会保存到文件中，重新打开此文件，新编码是什么则是什么。③含有中文的代码文件，Convert 之后可能会使中文变成乱码，所以在转换成请做好备份，不然可能出现转换过程变成乱码，无法还原。 设置省点模式IntelliJ IDEA 有一种叫做 省电模式 的状态，开启这种模式之后 IntelliJ IDEA 会关掉代码检查和代码提示等功能。所以一般也可认为这是一种 阅读模式，如果你在开发过程中遇到突然代码文件不能进行检查和提示，可以来看看这里是否有开启该功能。 变量类型提示(Scala开发中比较方便)在编写代码的过程中如果我们使用简写方式,开启该功能之后IDEA会自动展示完整的形式,包括变量的类型 IDEA常用快捷键123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108alt +enter 导包ctrl + x 剪切Ctrl + d 快速复制Ctrl + c 复制Ctrl + y 删除alt + F4 关闭IDEActrl + shift + F 全文搜索 类似eclipse中的ctrl+Hctrl +alt +l 格式化 (L的小写)Ctrl + / 或 ctrl+shift +/ 注释Ctrl + Alt + v 生成变量名 定义变量更方便或者在行尾添加.var 回车，生成变量名，并修改之 alt + t 可控制显示类型shift +f6 重命名ctrl +o 实现方法ctrl +i 重写方法double shift 查找ctrl +n 查找类shift + Enter 在当前行下另起一行ctrl+shift + 上下箭头 往上下移动导入main方法的快捷键 psvm 回车 适用于java代码----------------------------------------------------------ctrl +alt +空格 代码提示ctrl+shift+F10 Run context configuration from editor 运行当前类的main方法shift+F10 Run 正常运行当前Application中显示的类shift+F9 Debug Debug当前Applicaiton显示的类Alt+Shift+F10 Select configuration and run 调出Run里面的所有ApplicationAlt+Shift+F9 Select configuration and debug 选择Application，并debugIDEA中输出System.out.println（）的快捷键是soutIDEA快捷键大全：CTRL+N 查找类CTRL+SHIFT+N 查找文件CTRL+SHIFT+ALT+N 查 找类中的方法或变量CIRL+B 找变量的来源CTRL+ALT+B 找所有的子类CTRL+SHIFT+B 找变量的 类CTRL+G 定位行CTRL+F 在当前窗口查找文本CTRL+SHIFT+F 在指定窗口查找文本CTRL+R 在 当前窗口替换文本CTRL+SHIFT+R 在指定窗口替换文本ALT+SHIFT+C 查找修改的文件CTRL+E 最 近打开的文件F3 向下查找关键字出现位置类SHIFT+F3 向上一个关键字出现位置F4 查找变量来源CTRL+ALT+F7 选 中的字符 查找工程出现的地方CTRL+SHIFT+O 弹出显示查找内容自动代码ALT+回车 导入包,自动修正CTRL+ALT+L 格式化代码CTRL+ALT+I 自 动缩进CTRL+ALT+O 优化导入的类和包ALT+INSERT 生成代码(如GET,SET方法,构造函数等)CTRL+E 或者ALT+SHIFT+C 最近更改的代码CTRL+SHIFT+SPACE 自动补全代码CTRL+空格 代码提示CTRL+ALT+SPACE 类 名或接口名提示CTRL+P 方法参数提示CTRL+J 自动代码CTRL+ALT+T 把选中的代码放在 TRY&#123;&#125; IF&#123;&#125; ELSE&#123;&#125; 里 复制快捷方式F5 拷贝文件快捷方式CTRL+D 复制行CTRL+X 剪 切,删除行CTRL+SHIFT+V 可以复制多个文本 高亮CTRL+F 选中的文字,高亮显示 上下跳到下一个或者上一个F2 或SHIFT+F2 高亮错误或警告快速定位CTRL+SHIFT+F7 高亮显示多个关键字. 其他快捷方式CIRL+shift+U 大小写切换CTRL+Z 倒退CTRL+SHIFT+Z 向 前CTRL+ALT+F12 资源管理器打开文件夹ALT+F1 查找文件所在目录位置SHIFT+ALT+INSERT 竖 编辑模式CTRL+/ 注释// CTRL+SHIFT+/ 注释/*...*/CTRL+W 选中代码，连续按会 有其他效果CTRL+B 快速打开光标处的类或方法ALT+ ←/→ 切换代码视图CTRL+ALT ←/→ 返回上次编辑的位置ALT+ ↑/↓ 在方法间快速移动定位SHIFT+F6 重构-重命名CTRL+H 显 示类结构图CTRL+Q 显示注释文档ALT+1 快速打开或隐藏工程面板CTRL+SHIFT+UP/DOWN 代码 向上/下移动。CTRL+UP/DOWN 光标跳转到第一行或最后一行下ESC 光标返回编辑框SHIFT+ESC 光 标返回编辑框,关闭无用的窗口F1 帮助 千万别按,很卡!CTRL+F4 非常重要 下班都用 若有更好的功能,欢迎在评论区补充^ - ^]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>IDEA</tag>
        <tag>快捷键</tag>
        <tag>配置方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS基本原理与工作机制（一）——初识HDFS]]></title>
    <url>%2Fpost%2FHDFS%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E4%B8%8E%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E5%88%9D%E8%AF%86HDFS%2F</url>
    <content type="text"><![CDATA[HDFS简介 HDFS 源于 Google 在2003年10月份发表的GFS（Google File System） 论文。 是 GFS 的一个克隆版本HDFS（Hadoop Distributed File System）是Hadoop项目的核心子项目，是分布式计算中数据存储管理的基础，是基于流数据模式访问和处理超大文件的需求而开发的，可以运行于廉价的商用服务器上。它所具有的高容错、高可靠性、高可扩展性、高获得性、高吞吐率等特征为海量数据提供了不怕故障的存储，为超大数据集（Large Data Set）的应用处理带来了很多便利。 HDFS存储数据的优缺点HDFS的优点 高容错性 数据自动保存多个副本。通过增加副本的形式，提高容错性。 某一个副本丢失以后，可以自动恢复，这是由 HDFS 内部机制实现的，我们不必关心。 适合批处理 通过移动计算而不是移动数据。 它会把数据位置暴露给计算框架。 适合大数据处理 处理数据达到 GB、TB、甚至PB级别的数据。 能够处理百万规模以上的文件数量，数量相当之大。 能够处理10K节点的规模。 流式文件访问 一次写入，多次读取。 文件一旦写入不能修改，只能追加。 能保证数据的一致性。 可构建在廉价机器上 通过多副本机制，提高可靠性。 提供了容错和恢复机制。比如某一个副本丢失，可以通过其它副本来恢复。 HDFS的缺点 无法低延时数据访问 无法毫秒级的来存储数据。 适合高吞吐率的场景，就是在某一时间内写入大量的数据。但是它在低延时的情况下是不行的，比如毫秒级以内读取数据 不适合小文件存储 存储大量小文件(这里的小文件是指小于HDFS系统的Block大小的文件（默认64M）)的话，它会占用 NameNode大量的内存来存储文件、目录和块信息。这样是不可取的，因为NameNode的内存总是有限的。 小文件存储的寻道时间会超过读取时间，它违反了HDFS的设计目标。 不支持并发写入、文件随机修改 一个文件只能有一个写，不允许多个线程同时写。 仅支持数据 append（追加），不支持文件的随机修改 以上内容来自：https://www.cnblogs.com/codeOfLife/p/5375120.html HDFS架构集群架构 HDFS 采用Master/Slave的架构来存储数据，这种架构主要由四个部分组成，分别为HDFS Client、NameNode、DataNode和Secondary NameNode。这里重点介绍Secondary NameNode，其他架构部分参考下文高可用集群架构 Secondary NameNode 并非 NameNode 的热备。当NameNode 挂掉的时候，它并不能马上替换 NameNode 并提供服务 SecondaryNameNode的工作情况： SecondaryNameNode会定期和NameNode通信，请求其停止使用EditLog文件，暂时将新的写操作写到一个新的文件edit.new上来，这个操作是瞬间完成，上层写日志的函数完全感觉不到差别； SecondaryNameNode通过HTTP GET方式从NameNode上获取到FsImage和EditLog文件，并下载到本地的相应目录下； SecondaryNameNode将下载下来的FsImage载入到内存，然后一条一条地执行EditLog文件中的各项更新操作，使得内存中的FsImage保持最新；这个过程就是EditLog和FsImage文件合并； SecondaryNameNode执行完（3）操作之后，会通过post方式将新的FsImage文件发送到NameNode节点上 NameNode将从SecondaryNameNode接收到的新的FsImage替换旧的FsImage文件，同时将edit.new替换EditLog文件，通过这个过程EditLog就变小了此处内容来自:https://blog.csdn.net/xjz729827161/article/details/79463140 高可用集群架构 HDFS HA（High Availability）是为了解决单点故障问题 HA集群设置两个名称节点，“活跃（Active）”和“待命（Standby）” 两种名称节点的状态同步，可以借助于一个共享存储系统来实现 一旦活跃名称节点出现故障，就可以立即切换到待命名称节点 Zookeeper确保一个名称节点在对外服务 名称节点维护映射信息，数据节点同时向两个名称节点汇报信息 主Master，整个Hadoop集群只能有一个 管理HDFS文件系统的命名空间 维护元数据信息 管理副本的配置和信息（默认三个副本） 处理客户端读写请求 Standby Name Node Active Name Node的热备节点 Active Name Node故障时可快速切换成新的Active Name Node 周期性同步edits编辑日志，定期合并fsimage与edits到本地磁盘 Journal Node 可以被Active Name Node和StandBy Name Node同时访问，用以支持Active Name Node高可用 Active Name Node在文件系统被修改时，会向Journal Node写入操作日志（edits） Standby Name Node同步Journal Node edits日志，使集群中的更新操作可以被共享和同步。 Data Node Slave 工作节点，集群一般会启动多个 负责存储数据块和数据块校验 执行客户端的读写请求 通过心跳机制定期向NameNode汇报运行状态和本地所有块的列表信息 在集群启动时DataNode项NameNode提供存储Block块的列表信息 Block数据块 HDSF固定的最小的存储单元（默认128M，可配置修改） 写入到HDFS的文件会被切分成Block数据块（若文件大小小于数据块大小，则不会占用整个数据块） 默认配置下，每个block有三个副本 Client 与Name Node交互获取文件的元数据信息 与Data Node，读取或者写入数据 通过客户端可以管理HDFS 更多详情见：https://blog.csdn.net/u010993514/article/details/83009822 HDFS副本存放策略namenode如何选择在哪个datanode 存储副本（replication）？这里需要对可靠性、写入带宽和读取带宽进行权衡。Hadoop对datanode存储副本有自己的副本策略，在其发展过程中一共有两个版本的副本策略，分别如下所示： 此处内容来自：https://www.cnblogs.com/codeOfLife/p/5375120.html Hadoop2.x新特性 引入了NameNode Federation，解决了横向内存扩展 引入了Namenode HA，解决了namenode单点故障 引入了YARN，负责资源管理和调度 增加了ResourceManager HA解决了ResourceManager单点故障 更多详细信息：https://www.cnblogs.com/codeOfLife/p/5375120.html]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>HA</tag>
        <tag>NameNode</tag>
        <tag>DataNode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Shell脚本一键部署Hadoop]]></title>
    <url>%2Fpost%2F%E4%BD%BF%E7%94%A8Shell%E8%84%9A%E6%9C%AC%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2Hadoop%2F</url>
    <content type="text"><![CDATA[测试环境Linux系统版本：CentOS 7 实现功能1、Java环境一键配置2、Hadoop单机版一键安装3、Hadoop伪分布式一键安装4、Hadoop集群部署5、伪分布式hadoop初始化6、集群设置SSH免密登录（使用hadoop用户操作） 脚本说明部署Hadoop环境过程比较繁琐，还容易出错，写了一个Shell脚本用来一键部署，废话不多说，下面直接上代码。 集群部署的方法暂时还没进行全面测试，后面测试后会持续更新 若在使用过程中遇到什么问题或者是有好的改进方案欢迎在评论区指出 脚本代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697#!/bin/bashJDKLINK='http://download.oracle.com/otn-pub/java/jdk/8u191-b12/2787e4a523244c269598db4e85c51e0c/jdk-8u191-linux-x64.rpm'HADOOPLINK='https://archive.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz'localIP=$(ip a | grep ens33 | awk '$1~/^inet.*/&#123;print $2&#125;' | awk -F '/' '&#123;print $1&#125;')ip_arrays=()#初始化环境installWget()&#123; echo '初始化安装环境....' wget if [ $? -ne 1 ]; then echo '开始下载wget' yum -y install wget fi&#125;#wget下载JDK进行安装installJDK()&#123; ls /usr/local | grep 'jdk.*[rpm]$' if [ $? -ne 0 ]; then echo '开始下载JDK.......' wget --no-check-certificate --no-cookies --header "Cookie: oraclelicense=accept-securebackup-cookie" $JDKLINK mv $(ls | grep 'jdk.*[rpm]$') /usr/local fi chmod 751 /usr/local/$(ls /usr/local | grep 'jdk.*[rpm]$') rpm -ivh /usr/local/$(ls /usr/local | grep 'jdk.*[rpm]$')&#125;#JDK环境变量配置pathJDK()&#123; #PATH设置 grep -q "export PATH=" /etc/profile if [ $? -ne 0 ]; then #末行插入 echo 'export PATH=$PATH:$JAVA_HOME/bin'&gt;&gt;/etc/profile else #行尾添加 sed -i '/^export PATH=.*/s/$/:\$JAVA_HOME\/bin/' /etc/profile fi grep -q "export JAVA_HOME=" /etc/profile if [ $? -ne 0 ]; then #导入配置 filename="$(ls /usr/java | grep '^jdk.*[^rpm | gz]$' | sed -n '1p')" sed -i "/^export PATH=.*/i\export JAVA_HOME=\/usr\/java\/$filename" /etc/profile sed -i '/^export PATH=.*/i\export JRE_HOME=$JAVA_HOME/jre' /etc/profile sed -i '/^export PATH=.*/i\export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar' /etc/profile #echo "export JAVA_HOME=/usr/java/$filename"&gt;&gt;/etc/profile #echo 'export JRE_HOME=$JAVA_HOME/jre'&gt;&gt;/etc/profile #echo 'export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar'&gt;&gt;/etc/profile else #替换原有配置 filename="$(ls /usr/java | grep '^jdk.*[^rpm | gz]$' | sed -n '1p')" sed -i "s/^export JAVA_HOME=.*/export JAVA_HOME=\/usr\/java\/$filename/" /etc/profile fi source /etc/profile&#125;#wget下载Hadoop进行解压(单机版)wgetHadoop()&#123; ls /usr/local | grep 'hadoop.*[gz]$' if [ $? -ne 0 ]; then echo '开始下载hadoop安装包...' wget $HADOOPLINK mv $(ls | grep 'hadoop.*gz$') /usr/local fi tar -zxvf /usr/local/$(ls | grep 'hadoop.*[gz]$') mv /usr/local/$(ls | grep 'hadoop.*[^gz]$') /usr/local/hadoop&#125;#hadoop环境变量配置pathHadoop()&#123; #PATH设置 grep -q "export PATH=" /etc/profile if [ $? -ne 0 ]; then #末行插入 echo 'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin'&gt;&gt;/etc/profile else #行尾添加 sed -i '/^export PATH=.*/s/$/:\$HADOOP_HOME\/bin:\$HADOOP_HOME\/sbin/' /etc/profile fi #HADOOP_HOME设置 grep -q "export HADOOP_HOME=" /etc/profile if [ $? -ne 0 ]; then #在PATH前面一行插入HADOOP_HOME sed -i '/^export PATH=.*/i\export HADOOP_HOME=\/usr\/local\/hadoop' /etc/profile else #修改文件内的HADOOP_HOME sed -i 's/^export HADOOP_HOME=.*/export HADOOP_HOME=\/usr\/local\/hadoop/' /etc/profile fi source /etc/profile&#125;#添加hadoop用户并设置权限hadoopUserAdd()&#123; echo '正在创建hadoop用户....' useradd hadoop echo '请设置hadoop用户密码....' passwd hadoop gpasswd -a hadoop root chmod 771 /usr chmod 771 /usr/local chown -R hadoop:hadoop /usr/local/hadoop&#125;#单机版hadoop配置installHadoop()&#123; installWget wgetHadoop pathHadoop hadoopUserAdd&#125;#伪分布式设置setHadoop()&#123;echo '&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt; &lt;!-- Put site-specific property overrides in this file. --&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;指定hadoop运行时产生文件的存储路径&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;description&gt;hdfs namenode的通信地址,通信端口&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt;'&gt;$HADOOP_HOME/etc/hadoop/core-site.xmlecho '&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt; &lt;!-- Put site-specific property overrides in this file. --&gt;&lt;!-- 该文件指定与HDFS相关的配置信息。需要修改HDFS默认的块的副本属性，因为HDFS默认情况下每个数据块保存3个副本，而在伪分布式模式下运行时，由于只有一个数据节点，所以需要将副本个数改为1；否则Hadoop程序会报错。 --&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;description&gt;指定HDFS存储数据的副本数目，默认情况下是3份&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hadoopdata/namenode&lt;/value&gt; &lt;description&gt;namenode存放数据的目录&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hadoopdata/datanode&lt;/value&gt; &lt;description&gt;datanode存放block块的目录&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;关闭权限验证&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt;'&gt;$HADOOP_HOME/etc/hadoop/hdfs-site.xml echo '&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt; &lt;!-- Put site-specific property overrides in this file. --&gt;&lt;!-- 在该配置文件中指定与MapReduce作业相关的配置属性，需要指定JobTracker运行的主机地址--&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;description&gt;指定mapreduce运行在yarn上&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt;'&gt;$HADOOP_HOME/etc/hadoop/mapred-site.xmlecho '&lt;?xml version="1.0"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;configuration&gt; &lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;description&gt;mapreduce执行shuffle时获取数据的方式&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt;'&gt;$HADOOP_HOME/etc/hadoop/yarn-site.xml echo 'localhost'&gt;$HADOOP_HOME/etc/hadoop/slaves sed -i 's/export JAVA_HOME=.*/\#&amp;/' $HADOOP_HOME/etc/hadoop/hadoop-env.sh sed -i "/#export JAVA_HOME=.*/a export JAVA_HOME=$JAVA_HOME" $HADOOP_HOME/etc/hadoop/hadoop-env.sh chown -R hadoop:hadoop $HADOOP_HOME&#125;#完全分布式设置setHadoop2()&#123;echo '&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt; &lt;!-- Put site-specific property overrides in this file. --&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;指定hadoop运行时产生文件的存储路径&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://'$1':9000&lt;/value&gt; &lt;description&gt;hdfs namenode的通信地址,通信端口&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt;'&gt;$HADOOP_HOME/etc/hadoop/core-site.xmlecho '&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt; &lt;!-- Put site-specific property overrides in this file. --&gt;&lt;!-- 该文件指定与HDFS相关的配置信息。需要修改HDFS默认的块的副本属性，因为HDFS默认情况下每个数据块保存3个副本，而在伪分布式模式下运行时，由于只有一个数据节点，所以需要将副本个数改为1；否则Hadoop程序会报错。 --&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;description&gt;指定HDFS存储数据的副本数目，默认情况下是3份&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hadoopdata/namenode&lt;/value&gt; &lt;description&gt;namenode存放数据的目录&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hadoopdata/datanode&lt;/value&gt; &lt;description&gt;datanode存放block块的目录&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.secondary.http.address&lt;/name&gt; &lt;value&gt;'$2':50090&lt;/value&gt; &lt;description&gt;secondarynamenode 运行节点的信息，和 namenode 不同节点&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;关闭权限验证&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt;'&gt;$HADOOP_HOME/etc/hadoop/hdfs-site.xmlecho '&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt; &lt;!-- Put site-specific property overrides in this file. --&gt;&lt;!-- 在该配置文件中指定与MapReduce作业相关的配置属性，需要指定JobTracker运行的主机地址--&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;description&gt;指定mapreduce运行在yarn上&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt;'&gt;$HADOOP_HOME/etc/hadoop/mapred-site.xmlecho '&lt;?xml version="1.0"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;configuration&gt; &lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;'$1'&lt;/value&gt; &lt;description&gt;yarn总管理器的IPC通讯地址&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;description&gt;mapreduce执行shuffle时获取数据的方式&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt;'&gt;$HADOOP_HOME/etc/hadoop/yarn-site.xml rm -rf $HADOOP_HOME/etc/hadoop/slaves touch $HADOOP_HOME/etc/hadoop/slaves int=0 while(( $&#123;int&#125;&lt;$&#123;#ip_arrays[*]&#125; )) do #echo "while is run" echo "$&#123;ip_arrays[$int]&#125;"&gt;&gt;$HADOOP_HOME/etc/hadoop/slaves if [ $? -ne 0 ] then echo '写入slaves配置失败' break fi let "int++" done sed -i 's/export JAVA_HOME=.*/\#&amp;/' $HADOOP_HOME/etc/hadoop/hadoop-env.sh sed -i "/#export JAVA_HOME=.*/a export JAVA_HOME=$JAVA_HOME" $HADOOP_HOME/etc/hadoop/hadoop-env.sh chown -R hadoop:hadoop $HADOOP_HOME&#125;#关闭防火墙stopFirewalld()&#123; systemctl stop firewalld systemctl disable firewalld&#125;#IP校验,返回值0校验合法，1不合法。checkIPAddr()&#123; echo $1|grep "^[0-9]\&#123;1,3\&#125;\.\([0-9]\&#123;1,3\&#125;\.\)\&#123;2\&#125;[0-9]\&#123;1,3\&#125;$" &gt; /dev/null; #IP地址必须为全数字 if [ $? -ne 0 ] then return 1 fi ipaddr=$1 a=`echo $ipaddr|awk -F . '&#123;print $1&#125;'` #以"."分隔，取出每个列的值 b=`echo $ipaddr|awk -F . '&#123;print $2&#125;'` c=`echo $ipaddr|awk -F . '&#123;print $3&#125;'` d=`echo $ipaddr|awk -F . '&#123;print $4&#125;'` for num in $a $b $c $d do if [ $num -gt 255 ] || [ $num -lt 0 ] #每个数值必须在0-255之间 then return 1 fi done return 0 &#125;#控制台输入集群IPipInput()&#123; echo "本机IP地址为：$localIP" int=0 echo '输入完成后，输入ip值为0可退出' while read -p "输入第`expr $&#123;int&#125; + 1`台的IP:" ip do if [ "$ip" == "0" ] then break fi checkIPAddr $ip if [ $? -eq 0 ] then ip_arrays[$int]=$ip #echo $int else echo '输入的IP不合法,重新进行配置....' ipInput fi let "int++" done&#125;#scp设置免密登录scpOutput()&#123; int=0 while(( $&#123;int&#125;&lt;$&#123;#ip_arrays[*]&#125; )) do scp -r ~/.ssh $&#123;ip_arrays[$int]&#125;:~/ let "int++" done&#125;#SSH免密登录setSSH()&#123; echo '---------------配置ssh免密登录----------------------' echo '------------一路回车即可生成秘钥--------------------' ssh-keygen -t rsa echo '----------秘钥生成完成，开始生成公钥----------------' echo '根据提示输入相应的信息' echo '----------------------------------------------------' echo 'Are you sure you want to continue connecting (yes/no)?' echo '------------------输入"yes"-------------------------' echo 'hadoop@localhost s password:' echo '--------------输入hadoop用户密码--------------------' ssh-copy-id localhost&#125;#控制台选择本机角色nameOrData()&#123; echo '--------------------------' echo '1、namenode' echo '2、datanode' read -p '请选择本机的角色[1-2]:' n case $n in 1) return 0 ;; 2) return 1 ;; *) echo '输入错误！！！' nameOrData ;; esac&#125;#配置hosts文件setHosts()&#123; echo '开始配置/etc/hosts文件' echo '本机IP地址为：'$localIP'' read -p '请输入本机主机名(hostname):' hostname echo -e ''$localIP'\t'$hostname''&gt;&gt;/etc/hosts echo '根据提示输入其他主机名(hostname)' echo '-----------------------------------' int=0 while(( $&#123;int&#125;&lt;$&#123;#ip_arrays[*]&#125; )) do echo 'IP：'$&#123;ip_arrays[$int]&#125;'' read -p "请输入主机名：" hostname echo -e ''$&#123;ip_arrays[$int]&#125;'\t'$hostname''&gt;&gt;/etc/hosts echo '-----------------------------------' let "int++" done&#125;#1、Java环境一键配置javaInstall()&#123; echo '开始检查本机环境' java -version if [ $? -ne 0 ]; then installWget echo '开始配置JDK，请耐心等待......' installJDK pathJDK java -version if [ $? -eq 0 ]; then echo 'JDK配置完成' else echo '安装失败，请重新尝试或手动安装' fi else echo '已经配置该环境' fi&#125;#2、Hadoop单机版一键安装hadoopInstall()&#123; javaInstall echo '开始检查本机环境' hadoop if [ $? -ne 0 ]; then installHadoop hadoop if [ $? -eq 0 ]; then echo 'hadoop单机版配置完成' else echo '安装失败，请重新尝试或手动安装' fi else echo '已经配置该环境' fi&#125;#3、Hadoop伪分布式一键安装hadoopInstall2()&#123; javaInstall echo '开始检查本机环境' hadoop if [ $? -ne 0 ]; then installHadoop hadoop if [ $? -eq 0 ]; then echo 'hadoop单机版配置完成，开始配置伪分布式' setHadoop stopFirewalld echo '配置完成....使用hadoop用户初始化' su hadoop else echo '安装失败，请重新尝试或手动安装' fi else echo 'hadoop单机版已经安装，开始配置伪分布式' setHadoop stopFirewalld echo '配置完成....使用hadoop用户初始化' su hadoop fi&#125;#4、Hadoop集群部署hadoopInstall3()&#123; nameOrData if [ $? -eq 0 ] then #记录IP echo '输入datanode的IP' ipInput #namenode配置 #1安装单机版hadoop hadoopInstall #2导入集群配置文件 echo '开始导入配置文件' setHadoop2 $&#123;localIP&#125; $&#123;ip_arrays[0]&#125; echo '配置导入完成' #3关闭防火墙 stopFirewalld echo '防火墙已关闭' #上传主机配置到datanode int=0 while(( $&#123;int&#125;&lt;$&#123;#ip_arrays[*]&#125; )) do echo "开始给第`expr $&#123;int&#125; + 1`台datanode传送配置文件和安装包" echo "IP为：$&#123;ip_arrays[$&#123;int&#125;]&#125;" echo "传送过程需手动输入远程主机root密码" #scp传送安装包 scp $(pwd)/install.sh $&#123;ip_arrays[$int]&#125;:/usr/local scp /usr/local/$(ls | grep 'jdk.*[rpm]$') $&#123;ip_arrays[$int]&#125;:/usr/local scp -r /usr/local/hadoop $&#123;ip_arrays[$int]&#125;:/usr/local echo "$&#123;ip_arrays[$int]&#125;文件上传完成....." let "int++" done setHosts echo '请登录datanode主机执行该脚本继续完成datanode配置，脚本存储目录/usr/local' elif [ $? -eq 1 ] then #安装Java javaInstall #配置Hadoop环境变量 echo '配置环境变量' pathHadoop echo '环境变量配置完成' #添加用户 hadoopUserAdd #关闭防火墙 stopFirewalld echo '防火墙已关闭' source /etc/profile echo '测试安装情况.....' java -version if [ $? -ne 0 ]; then echo '请手动执行source /etc/profile' echo '执行java -version确认JDK安装情况' fi hadoop version if [ $? -ne 0 ]; then echo '请手动执行source /etc/profile' echo '执行hadoop version确认hadoop安装情况' fi echo 'datanode配置完成' else echo '发生错误！！！' fi&#125;#6、集群设置SSH免密登录（使用hadoop用户操作）setSSHS()&#123; #本机设置免密 echo '开始设置本机免密....' setSSH #输入其他电脑IP echo '开始设置其他主机....' echo '输入其他主机ip' ipInput #用scp将秘钥发到其他主机 echo '开始发送秘钥到其他主机...' scpOutput&#125;#控制台输入选项consoleInput()&#123; echo '请输入选项[1-4]' echo '1、Java环境一键配置' echo '2、Hadoop单机版一键安装' echo '3、Hadoop伪分布式一键安装' echo '4、Hadoop集群部署' echo '5、hadoop初始化（在namenode主机上执行）' echo '6、集群设置SSH免密登录（使用hadoop用户操作）' echo '请输入选项[1-6]' read aNum case $aNum in 1) javaInstall ;; 2) hadoopInstall ;; 3) hadoopInstall2 ;; 4) hadoopInstall3 ;; 5) echo 'Hadoop初始化' hdfs namenode -format ;; 6) setSSHS ;; *) echo '没有该选项，请重新输入!!!退出请按Ctrl+c' consoleInput ;; esac&#125;echo '------------------欢迎使用一键安装------------------'echo '为保证安装过程顺利进行，请使用root用户执行该脚本'echo '该脚本增加了本地安装包自动安装'echo '如果需要脚本安装本地安装包，请将安装包放在/usr/local下'echo 'hadoop安装包要求以hadoop开头的.tar.gz包'echo 'JDK安装包要求以jdk开头的.rpm包'echo '----------------------------------------------------'consoleInput]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Linux</tag>
        <tag>Hadoop部署</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7常用基本命令整理]]></title>
    <url>%2Fpost%2FCentOS-7%E5%B8%B8%E7%94%A8%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[重启命令 立刻重启(root用户使用) 12345rebootshutdown -r nowinit 6 10分钟后自动重启(root用户使用) 1shutdown -r 10 在时间为20:35时候重启(root用户使用) 1shutdown -r 20:35 如果是通过shutdown命令设置重启的话，可以取消重启 1shutdown -c 关机命令 立刻关机(root用户使用) 1234567haltpoweroffshutdown -h nowinit 0 10分钟后自动关机 1shutdown -h 10 hostname命令查看主机名1hostname 设置主机名 临时修改 1hostname 主机名 永久修改 1vi /etc/hostname 网络服务 查看IP信息 1ip a 网络连通性测试 1ping [选项] 目标主机 设置网络信息 1vi /etc/sysconfig/network-scripts/ifcfg-ens33 重启network网络服务 1service network restart 防火墙设置 查看防火墙状态 1systemctl status firewalld 关闭防火墙 1systemctl start firewalld 禁止开机启动 1systemctl disable firewalld 主机映射文件 修改主机名与IP映射关系1vi /etc/hosts 目录操作命令 查看工作目录（Print Working Directory） 1pwd 切换工作目录（Change Directory） 1cd [目录位置] 列表（List）显示目录内容 1ls [选项]... [目录或文件名] 常用命令选项 -l ：详细信息显示 -a：显示所有子目录和文件的信息，包括隐藏文件 -A：类似于“-a”，但不显示“.”和“..”目录的信息 -R：递归显示内容 创建新的目录（Make Directory） 1mkdir [-p] [/路径/]目录名 统计目录及文件的空间占用情况（estimate file space usage） 1du [选项]... [目录或文件名] 常用命令选项 -a：统计时包括所有的文件，而不仅仅只统计目录 -h：以更易读的字节单位（K、M等）显示信息 -s：只统计每个参数所占用空间总的大小 文件操作命令 新建空文件，或更新文件时间标记 1touch 文件名 查看文件类型 1file 文件名 复制（copy）文件或目录 1cp [选项] 源文件或目录… 目标文件或目录 常用命令选项 -r：递归复制整个目录树 -p：保持源文件的属性不变 -f：强制覆盖目标同名文件或目录 -i：需要覆盖文件或目录时进行提醒 删除（Remove）文件或目录1rm [选项] 文件或目录 常用命令选项 -f：强行删除文件，不进行提醒 -i：删除文件时提醒用户确认 -r：递归删除整个目录树 移动（Move）文件或目录1mv [选项]... 源文件或目录… 目标文件或目录 如果目标位置与源位置相同，则相当于改名 显示系统命令所在目录1which &lt;选项&gt; command（命令名称） 常用命令选项 -a：将所有由PATH路径中可以找到的指令均列出，而不止第一个被找到的指令名称 find查找1find &lt;路径&gt; &lt;选项&gt; [表达式] find查找的特点从指定路径下递归向下搜索文件支持按照各种条件方式查找支持对查找到的文件再进一步的使用指令操作（例如：删除、统计大小、复制等） 常用命令选项 -name 根据文件名查找 -user 根据文件拥有者查找 -group 根据文件所属组寻找文件 -perm 根据文件权限查找文件 -size 根据文件大小查找文件 -type 根据文件类型查找（f-普通文件，c-字符设备文件，b-块设备文件，l-链接文件，d-目录） -o 表达式或 -and 表达式与 文件内容操作命令 显示出文件的全部内容 1cat 全屏方式分页显示文件内容 1more 交互操作方法按Enter键向下逐行滚动按空格键向下翻一屏、按b键向上翻一屏按q键退出 与more命令相同 1less 查看文件开头的一部分内容（默认为10行） 1head -n 文件名 查看文件结尾的少部分内容（默认为10行） 1tail -n 文件名 统计文件中的单词数量（Word Count）等信息 1wc [选项] 目标文件 常用命令选项-l：统计行数-w：统计单词个数-c：统计字节数 查找文件里符合条件的字符串1grep [选项] &lt;关键字&gt; &lt;文件…&gt; 常用选项-c:计算匹配关键字的行数-i:忽略字符大小写的差别-n:显示匹配的行及其行号-s:不显示不存在或不匹配文本的错误信息-h: 查询多个文件时不显示文件名-l:查询文件时只显示匹配字符所在的文件名–color=auto:将找到的关键字部分加上颜色显示 压缩命令 压缩（解压）文件或目录，压缩文件后缀为gz1gzip [选项] 压缩（解压缩）的文件名 常用选项-d将压缩文件解压（decompress）-l显示压缩文件的大小，未压缩文件的大小，压缩比（list）-v显示文件名和压缩比（verbose）-num用指定的数字num调整压缩的速度，-1或–fast表示最快压缩方法（低压缩比），-9或–best表示最慢压缩方法（高压缩比）。系统缺省值为6 压缩（解压）文件或目录，压缩文件后缀为bz21bzip2 [-cdz] 文档名 常用选项-c将压缩的过程产生的数据输出到屏幕上-d解压缩的参数（decompress）-z压缩的参数（compress）-num 用指定的数字num调整压缩的速度，-1或–fast表示最快压缩方法（低压缩比），-9或–best表示最慢压缩方法（高压缩比）。系统缺省值为6 压缩、解压文件12tar [cvf]... 压缩名 文件名（压缩文件）tar [xvf]... 文件名.tar（解压文件） 常用命令选项-c：创建 .tar 格式的包文件-x：解开.tar格式的包文件-v：输出详细信息-f：表示使用归档文件 文本编辑器 vi编辑器类Unix系统中默认的文本编辑器vi可以执行输出、删除、查找、替换、块操作等众多文本操作， 而且用户可以根据自己的需要对其进行定制维护Linux系统中的各种配置文件 vim编辑器vi编辑器的增强版本，习惯上也称为vi 插入命令 定位命令 删除命令dd:删除当前行ndd:删除光标所在当前行向下数n行D:删除当前行光标所在的位置后面的字符x:向后删除光标所在位置的字符X:向前删除光标前面的字符nX:删除前面的n个字符，光标所在的字符将不会被删 复制和粘贴命令yy或Y:复制当前行nyy或nY：复制以下n行p:在光标后面插入buffer中的内容P:在光标前面插入buffer中的内容 替换和撤销命令r:取代光标所在处的字符R:从光标所在处开始替换字符，按esc结束u:撤销上一步操作 定位命令h:左移一个字符l:右移一个字符j:下移一行k:上移一行$:移至行尾0:移至行首nG:移到第n行 替换操作: s /old/new 将当前行中查找到的第一个字符“old” 串替换为“new”: s /old/new/g 将当前行中查找到的所有字符串“old” 替换为“new”:#,# s/old/new/g 在行号“#,#”范围内替换所有的字符串“old”为“new”:% s/old/new/g 在整个文件范围内替换所有的字符串“old”为“new”:%s/old/new 查找文件中所有行第一次出现的old，替换为new 其他命令:W[文件路径]保存当前文件:q 如果未对文件做改动则退出:q! 放弃存储名退出:wq或:x 保存退出可视模式v：可视模式V：可视行模式Ctrl+v：可视块模式注意：在所有可视模式中，d和x键可以用删除选定的内容在可视块模式中，选中所需行，按I键输入内容，之后按两次esc键，可在所有选定行光标处添加同样的内容。用户和组配置文件 保存用户信息的文件：1/etc/passwd 用于保存用户的帐号基本信息每一行对应一个用户的帐号记录,一行有7个段位，用“：”隔开 保存密码的文件：1/etc/shadow 用于保存密码字串、密码有效期等信息每一行对应一个用户的密码记录 保存用户组的文件：1/etc/group 保存组账号基本信息文件每一行表示一组记录，包括组名、GID和组的成员，（组成员显示次组成员） 保存用户组密码的文件：1/etc/gshadow 保存组帐号的密码信息用户组用户组密码，如果是空或者有“！”，表示没有密码用户组管理者组成员，用逗号“，”隔开 用户配置文件： 1/etc/default/useradd 用户角色root用户，系统唯一，可以操作系统任何文件和命令，拥有最高权限，UID=0虚拟用户(系统账户)，不具有登录系统能力，但却是系统运行不可缺少的用户。如：bin、daemon、ftp、mail等，UID为1—499之间普通真实用户，可以登录系统，权限有限，靠管理员创建，UID为500—60000之间 用户管理 添加用户命令1useradd -u 指定组ID（uid）-g 指定所属的组名（gid）-G 指定多个组，用逗号“，”分开（Groups）-c 用户描述（comment）-e 失效时间（expire date） 设置密码1passwd [选项] &lt;用户名&gt; -d：清空用户的密码，使之无需密码即可登录-l：锁定用户帐号-S：查看用户帐号的状态（是否被锁定）-u：解锁用户帐号-x: 最大密码使用时间（天）-n: 最小密码使用时间（天） 修改用户命令 1usermod -l 修改用户名 （login）usermod -l a b（b改为a）-g 添加组 usermod -g sys tom-G添加多个组 usermod -G sys,root tom–L 锁定用户账号密码（Lock）–U 解锁用户账号（Unlock） 删除用户命令 1userdel [选项] 用户名 -r 删除账号时同时删除目录（remove） 组管理 添加组1groupadd -g 指定gid 修改用户组的属性1groupmod [选项] &lt;用户名&gt; -g：设置想要使用的GID-o：使用已经存在的GID-n：设置想要使用的群组名称 添加/删除组成员1gpasswd [选项] 组帐号名 -a：向组内添加一个用户-d：从组内删除一个用户成员-M：定义组成员列表，以逗号分隔 删除组账号1groupdel &lt;组账号名&gt; 只能删除那些没有被任何用户指定为主组的组 显示用户所属组1groups [用户名] 权限和权限值读( r )：读取文件的内容；列出目录里的对象写( w )：允许修改文件；在目录里面新建或者删除文件执行( x )：允许执行文件；允许进入目录里 除了用字母rwx来表示权限，还可以使用3位数字来表 达文件或目录的权限读：4写：2执行：1 chmod命令1chmod [ugoa] [+-=] [rwx] file/dir 或 chmod nnn file/dir u:属主 g:属组 o:其他用户 a:所有用户+:添加权限 -:删除权限 =:赋予权限nnn:三位八进制的权限-R 递归修改指定目录下的所有子文件及文件夹的权限-f 强制改变文件访问特权；如果是文件的拥有者，则得 不到任何错误信息 chown命令123chown 属主 file/dir chown :属组 file/dirchown 属主:属组 file/dir -R：递归的修改指定目录下所有文件、子目录的归属 软件包管理RPM命令使用1rpm -i：安装应用程序（install）-e：卸载应用程序（erase）-vh：显示安装进度；（verbose hash）-U：升级软件包；（update）-qa: 显示所有已安装软件包（query all） YUM命令Yum（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及SUSE、CentOS中的Shell前端软件包管理器。基於RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。123yum install gcc-c++yum remove gcc-c++yum update gcc-c++]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7部署Hadoop集群（HA高可用集群）]]></title>
    <url>%2Fpost%2FCentOS-7%E9%83%A8%E7%BD%B2Hadoop%E9%9B%86%E7%BE%A4%EF%BC%88HA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%89%2F</url>
    <content type="text"><![CDATA[测试环境Linux系统版本：CentOS 7 64位 Hadoop版本：hadoop-2.7.3 Java版本：jdk-8u181-linux-x64 ZooKeeper版本：zookeeper-3.4.10.tar.gz 配置HA高可用集群建议先看一下完全分布式集群的部署过程，整个流程大致一样。 CentOS 7部署Hadoop集群（完全分布式） Hadoop 组织框架Hadoop主要包括两部分： 一部分是HDFS（Hadoop Distributed File System），主要负责分布式存储和计算； 另一部分是YARN（Yet Another Resource Negotiator， 从Hadoop2.0开始引入），主要负责集群的资源管理和调度。 HDFS架构 架构图： 1. Active Name Node 主Master，整个Hadoop集群只能有一个 管理HDFS文件系统的命名空间 维护元数据信息 管理副本的配置和信息（默认三个副本） 处理客户端读写请求 2. Standby Name Node Active Name Node的热备节点 Active Name Node故障时可快速切换成新的Active Name Node 周期性同步edits编辑日志，定期合并fsimage与edits到本地磁盘 3. Journal Node 可以被Active Name Node和StandBy Name Node同时访问，用以支持Active Name Node高可用 Active Name Node在文件系统被修改时，会向Journal Node写入操作日志（edits） Standby Name Node同步Journal Node edits日志，使集群中的更新操作可以被共享和同步。 4. Data Node Slave 工作节点，集群一般会启动多个 负责存储数据块和数据块校验 执行客户端的读写请求 通过心跳机制定期向NameNode汇报运行状态和本地所有块的列表信息 在集群启动时DataNode项NameNode提供存储Block块的列表信息 5. Block数据块 HDSF固定的最小的存储单元（默认128M，可配置修改） 写入到HDFS的文件会被切分成Block数据块（若文件大小小于数据块大小，则不会占用整个数据块） 默认配置下，每个block有三个副本 6. Client 与Name Node交互获取文件的元数据信息 与Data Node，读取或者写入数据 通过客户端可以管理HDFS YARN架构 架构图： 1. Resource Manager 整个集群只有一个Master。Slave可以有多个，支持高可用 处理客户端Client请求 启动／管理／监控ApplicationMaster 监控NodeManager 资源的分配和调度 2. Node Manager 每个节点只有一个，一般与Data Node部署在同一台机器上且一一对应 定时向Resource Manager汇报本机资源的使用状况 处理来自Resource Manager的作业请求，为作业分配Container 处理来自Application Master的请求，启动和停止Container 3. Application Master 每个任务只有一个，负责任务的管理，资源的申请和任务调度 与Resource Manager协商，为任务申请资源 与Node Manager通信，启动／停止任务 监控任务的运行状态和失败处理 4. Container 任务运行环境的抽象，只有在分配任务时才会抽象生成一个Container 负责任务运行资源和环境的维护（节点，内存，CPU） 负责任务的启动 虽然在架构图中没有画出，但Hadoop高可用都是基于Zookeeper来实现的。如NameNode高可用，Block高可用，ResourceManager高可用等 以上部分内容来自：https://baijiahao.baidu.com/s?id=1589175554246101619&amp;wfr=spider&amp;for=pc HA集群部署规划 主机名称 IP地址 用户名称 进程 安装的软件 node200 192.168.33.200 hadoop NameNode（Active）、ResourceManager（StandBy）、ZKFC、JobHistoryServer JDK、Hadoop node201 192.168.33.201 hadoop NameNode（StandBy）、ResourceManager（Active）、ZKFC、WebProxyServer JDK、Hadoop node202 192.168.33.202 hadoop DataNode、NodeManager、JournalNode、QuorumPeerMain JDK、Hadoop、Zookeeper node203 192.168.33.203 hadoop DataNode、NodeManager、JournalNode、QuorumPeerMain JDK、Hadoop、Zookeeper node204 192.168.33.204 hadoop DataNode、NodeManager、JournalNode、QuorumPeerMain JDK、Hadoop、Zookeeper 规划说明： HDFS HA通常由两个NameNode组成，一个处于Active状态，另一个处于Standby状态。 Active NameNode对外提供服务，而Standby NameNode则不对外提供服务，仅同步Active NameNode的状态，以便能够在它失败时快速进行切换。 Hadoop 2.0官方提供了两种HDFS HA的解决方案，一种是NFS，另一种是QJM。这里我们使用简单的QJM。在该方案中，主备NameNode之间通过一组JournalNode同步元数据信息，一条数据只要成功写入多数JournalNode即认为写入成功。通常配置奇数个JournalNode，这里还配置了一个Zookeeper集群，用于ZKFC故障转移，当Active NameNode挂掉了，会自动切换Standby NameNode为Active状态。 YARN的ResourceManager也存在单点故障问题，这个问题在hadoop-2.4.1得到了解决：有两个ResourceManager，一个是Active，一个是Standby，状态由Zookeeper进行协调。 YARN框架下的MapReduce可以开启JobHistoryServer来记录历史任务信息，否则只能查看当前正在执行的任务信息。 Zookeeper的作用是负责HDFS中NameNode主备节点的选举，和YARN框架下ResourceManaer主备节点的选举。 部分内容来自：https://www.linuxidc.com/Linux/2016-08/134180.htm 自动故障转移 Zookeeper集群作用： 一是故障监控。每个NameNode将会和Zookeeper建立一个持久session，如果NameNode失效，那么此session将会过期失效，此后Zookeeper将会通知另一个Namenode，然后触发Failover； 二是NameNode选举。ZooKeeper提供了简单的机制来实现Acitve Node选举，如果当前Active失效，Standby将会获取一个特定的排他锁，那么获取锁的Node接下来将会成为Active。 ZKFC： ZKFC是一个Zookeeper的客户端，它主要用来监测和管理NameNodes的状态，每个NameNode机器上都会运行一个ZKFC程序 主要职责： 一是健康监控。ZKFC间歇性的ping NameNode，得到NameNode返回状态，如果NameNode失效或者不健康，那么ZKFS将会标记其为不健康； 二是Zookeeper会话管理。当本地NaneNode运行良好时，ZKFC将会持有一个Zookeeper session，如果本地NameNode为Active，它同时也持有一个“排他锁”znode，如果session过期，那么次lock所对应的znode也将被删除； 三是选举。当集群中其中一个NameNode宕机，Zookeeper会自动将另一个激活。 此处内容来自：https://www.linuxidc.com/Linux/2016-08/134180.htm 关于集群主机时间因为高可用集群的机制，各主机在集群中的时间需一致。 在下面Linux搭建前将虚拟机进行设置，设置方法如下： 安装完成后对每台主机的时间进行确认，确保每台主机时间一致。 Linux环境搭建按如下方法部署五台主机，主机名与IP地址的对应关系见上文集群部署规划 VMware虚拟机安装Linux系统 配置完成之后各主机IP、主机名与时间信息如下：（时间不一致的自己百度同步集群时间的方法） 命令：1234#查看系统ip信息ip a#查看系统时间date 执行结果：123456789101112131415[root@node200 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:6a:0e:74 brd ff:ff:ff:ff:ff:ff inet 192.168.33.200/24 brd 192.168.33.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe6a:e74/64 scope link valid_lft forever preferred_lft forever[root@node200 ~]# date2018年 10月 11日 星期四 16:57:56 CST 123456789101112131415[root@node201 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:60:36:3c brd ff:ff:ff:ff:ff:ff inet 192.168.33.201/24 brd 192.168.33.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe60:363c/64 scope link valid_lft forever preferred_lft forever[root@node201 ~]# date2018年 10月 11日 星期四 16:57:48 CST 123456789101112131415[root@node202 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:41:4a:f4 brd ff:ff:ff:ff:ff:ff inet 192.168.33.202/24 brd 192.168.33.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe41:4af4/64 scope link valid_lft forever preferred_lft forever[root@node202 ~]# date2018年 10月 11日 星期四 16:58:00 CST 123456789101112131415[root@node203 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:53:27:40 brd ff:ff:ff:ff:ff:ff inet 192.168.33.203/24 brd 192.168.33.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe53:2740/64 scope link valid_lft forever preferred_lft forever[root@node203 ~]# date2018年 10月 11日 星期四 16:58:01 CST 123456789101112131415[root@node204 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:6e:ad:fa brd ff:ff:ff:ff:ff:ff inet 192.168.33.204/24 brd 192.168.33.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe6e:adfa/64 scope link valid_lft forever preferred_lft forever[root@node204 ~]# date2018年 10月 11日 星期四 16:57:54 CST 网络测试： 配置完成之后测试各主机网络互通情况，在每台主机上执行下面两条命令，运行过程中按Ctrl+C可以终止进程，下面就不贴测试效果了12ping 192.168.33.1ping www.baidu.com 配置Java环境node200、node201、node202、node203、node204都需要安装 为上面安装的系统配置Java环境变量，本文中就写关键配置步骤与执行命令了，想了解详细的配置过程可以查看： Linux系统下安装Java环境 为了方便，本文就直接使用rpm包安装了，/etc/profile文件暂时不进行配置，到后面配置hadoop单机版时再进行配置 [1-3]均使用root用户执行 1、将安装包jdk-8u181-linux-x64.rpm上传到/usr/local目录下 2、安装rpm包，先设置权限，然后执行rpm命令安装12chmod 755 /usr/local/jdk-8u181-linux-x64.rpmrpm -ivh /usr/local/jdk-8u181-linux-x64.rpm 3、校验安装情况1java -version 安装单机版Hadoopnode200、node201、node202、node203、node204都需要安装 详细步骤查看：Hadoop部署（三）——CentOS 7部署Hadoop（单机版），这里只简单介绍安装步骤 [1-5]均使用root用户执行 1、将压缩包hadoop-2.7.3.tar.gz上传到/usr/local目录下 2、解压压缩包，进入/usr/local目录，对文件夹重命名123tar -zxvf /usr/local/hadoop-2.7.3.tar.gzcd /usr/localmv hadoop-2.7.3 hadoop 3、创建hadoop用户和hadoop用户组，并设置hadoop用户密码12useradd hadooppasswd hadoop 4、为hadoop用户添加sudo权限1vi /etc/sudoers 在root用户下面一行加上hadoop ALL=(ALL) ALL，保存并退出（这里需要用wq!强制保存退出）123456789101112## Next comes the main part: which users can run what software on## which machines (the sudoers file can be shared between multiple## systems).## Syntax:#### user MACHINE=COMMANDS#### The COMMANDS section may have other options added to it.#### Allow root to run any commands anywhereroot ALL=(ALL) ALLhadoop ALL=(ALL) ALL 5、将hadoop文件夹的主：组设置成hadoop，/usr目录与/usr/local目录所属主：组均为root，默认权限为755，也就是说其他用户（hadoop）没有写入（w）权限，在这里我们需要将这两个目录其他用户的权限设置为7123chown -R hadoop:hadoop hadoopchmod 757 /usrchmod 757 /usr/local Zookeeper集群安装在node202、node203、node204上安装 Zookeeper是一个开源分布式协调服务，其独特的Leader-Follower集群结构，很好的解决了分布式单点问题。目前主要用于诸如：统一命名服务、配置管理、锁服务、集群管理等场景。大数据应用中主要使用Zookeeper的集群管理功能。 [1-5]均使用root用户执行 1、将压缩包zookeeper-3.4.10.tar.gz上传到node202的/usr/local目录下 2、解压压缩包，进入/usr/local目录，对文件夹重命名123tar -zxvf /usr/local/zookeeper-3.4.10.tar.gzcd /usr/localmv zookeeper-3.4.10 zookeeper 3、修改zookeeper的配置文件，命令如下：123cd /usr/local/zookeeper/conf/cp zoo_sample.cfg zoo.cfgvi zoo.cfg 配置文件如下：1234567891011121314151617181920212223# 客户端心跳时间(毫秒)tickTime=2000# 允许心跳间隔的最大时间initLimit=10# 同步时限syncLimit=5# 数据存储目录dataDir=/usr/local/zookeeperdata# 数据日志存储目录dataLogDir=/usr/local/tmp/zookeeperlogs# 端口号clientPort=2181# 集群节点和服务端口配置server.1=node202:2888:3888server.2=node203:2888:3888server.3=node204:2888:3888# 以下为优化配置# 服务器最大连接数，默认为10，改为0表示无限制maxClientCnxns=0# 快照数autopurge.snapRetainCount=3# 快照清理时间，默认为0autopurge.purgeInterval=1 3、修改完zookeeper的配置文件，将node202上的zookeeper上传到其他两台服务器：1234567[root@node202 conf]# scp -r /usr/local/zookeeper 192.168.33.203:/usr/local/[root@node202 conf]# scp -r /usr/local/zookeeper 192.168.33.204:/usr/local/ #这儿输入yes回车Are you sure you want to continue connecting (yes/no)? yes#这里输入远程主机的root密码root@192.168.33.204's password: 4、在三台主机上创建zookeeper数据存储目录：1mkdir /usr/local/zookeeperdata 在文件夹下面创建一个文件，叫myid，并且在文件里写入server.X对应的X1234# 集群节点和服务端口配置server.1=node202:2888:3888server.2=node203:2888:3888server.3=node204:2888:3888 上述配置中node202是1，node203是2，node204是3，所以按如下操作： 在node202 上执行：1echo "1" &gt; /usr/local/zookeeperdata/myid 在node203 上执行：1echo "2" &gt; /usr/local/zookeeperdata/myid 在node204 上执行：1echo "3" &gt; /usr/local/zookeeperdata/myid 5、将zookeeper文件夹的主：组设置成hadoop12chown -R hadoop:hadoop /usr/local/zookeeperchown -R hadoop:hadoop /usr/local/zookeeperdata 配置环境变量node200、node201、node202、node203、node204都需要配置，第2步有所区别 [1-3]均使用root用户执行 1、编辑/etc/profile文件1vi /etc/profile 2、在末尾加上如下代码 node200、node201添加如下代码：12345export JAVA_HOME=/usr/java/jdk1.8.0_181-amd64export HADOOP_HOME=/usr/local/hadoopexport PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JRE_HOME=$JAVA_HOME/jre node202、node203、node204添加如下代码：123456export JAVA_HOME=/usr/java/jdk1.8.0_181-amd64export HADOOP_HOME=/usr/local/hadoopexport ZOOKEEPER_HOME=/usr/local/zookeeperexport PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZOOKEEPER_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JRE_HOME=$JAVA_HOME/jre 3、配置完环境变量之后保存退出，让环境变量立即生效1source /etc/profile 关闭防火墙node200、node201、node202、node203、node204都需要配置 CentOS 7使用的是firewalld作为防火墙，与CentOS 6 有所不同 下面三步均使用root用户执行 查看防火墙状态：1systemctl status firewalld 关闭防火墙：1systemctl stop firewalld 关闭防火墙开机自动启动：1systemctl disable firewalld 更多详情可以了解：CentOS 7部署Hadoop（伪分布式） 修改hosts文件修改所有主机的/etc/hosts文件，这里使用root用户操作1vi /etc/hosts 在文件最后面加上：12345192.168.33.200 node200192.168.33.201 node201192.168.33.202 node202192.168.33.203 node203192.168.33.204 node204 注意：IP后面是Tab制表符，而不是空格，这里配置完后最好测试一下网络。如果复制粘贴配置完后无法ping通，可能是IP地址后面的空格问题。 以下为测试方法：12345ping node200ping node201ping node202ping node203ping node204 配置到这里，我们切换到hadoop用户，使用hadoop用户进行下面的操作 配置SSH免密登录具体方法参照： Linux系统配置SSH免密登录(多主机互通) 这里贴关键代码，不展示操作过程： 在node200、node201、node202、node203、node204上分别操作，一路回车就可以了：12345[hadoop@node200 ~]$ ssh-keygen -t rsa[hadoop@node201 ~]$ ssh-keygen -t rsa[hadoop@node202 ~]$ ssh-keygen -t rsa[hadoop@node203 ~]$ ssh-keygen -t rsa[hadoop@node204 ~]$ ssh-keygen -t rsa 在node200上操作：12345[hadoop@node200 ~]$ ssh-copy-id localhost[hadoop@node200 ~]$ scp ~/.ssh/* node201:~/.ssh[hadoop@node200 ~]$ scp ~/.ssh/* node202:~/.ssh[hadoop@node200 ~]$ scp ~/.ssh/* node203:~/.ssh[hadoop@node200 ~]$ scp ~/.ssh/* node204:~/.ssh 修改Hadoop配置文件均使用hadoop用户操作，只需要在++node200++上修改即可 先进入/usr/local/hadoop/etc/hadoop/文件1[hadoop@node200 ~]$ cd /usr/local/hadoop/etc/hadoop/ 1、修改hadoop-env.sh文件1[hadoop@node200 hadoop]$ vi hadoop-env.sh 找到export JAVA_HOME=${JAVA_HOME}，在前面加个#注释掉，将JAVA_HOME用路径代替，如下：12#export JAVA_HOME=$&#123;JAVA_HOME&#125;export JAVA_HOME=/usr/java/jdk1.8.0_181-amd64 2、修改core-site.xml文件1[hadoop@node200 hadoop]$ vi core-site.xml 配置文件如下：1234567891011121314151617181920212223242526272829303132333435363738394041&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt; &lt;!-- Put site-specific property overrides in this file. --&gt; &lt;configuration&gt; &lt;!-- 指定hdfs的nameservices名称为mycluster，与hdfs-site.xml的HA配置相同 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定缓存文件存储的路径 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/tmp/hadoop/data&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置hdfs文件被永久删除前保留的时间（单位：分钟），默认值为0表明垃圾回收站功能关闭 --&gt; &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址，配置HA时需要 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node202:2181,node203:2181,node204:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3、修改hdfs-site.xml1[hadoop@node200 hadoop]$ vi hdfs-site.xml 配置文件如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt; &lt;!-- Put site-specific property overrides in this file. --&gt; &lt;configuration&gt; &lt;!-- 指定hdfs元数据存储的路径 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoopdata/namenode&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hdfs数据存储的路径 --&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoopdata/datanode&lt;/value&gt; &lt;/property&gt; &lt;!-- 数据备份的个数 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- 关闭权限验证 --&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启WebHDFS功能（基于REST的接口服务） --&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- //////////////以下为HDFS HA的配置////////////// --&gt; &lt;!-- 指定hdfs的nameservices名称为mycluster --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定mycluster的两个namenode的名称分别为nn1,nn2 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置nn1,nn2的rpc通信端口 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;node200:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;node201:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置nn1,nn2的http通信端口 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;node200:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;node201:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定namenode元数据存储在journalnode中的路径 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://node202:8485;node203:8485;node204:8485/mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定journalnode日志文件存储的路径 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/usr/local/tmp/journalnodelogs&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定HDFS客户端连接active namenode的java类 --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置隔离机制为ssh --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定秘钥的位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启自动故障转移 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4、修改mapred-site.xml /usr/local/hadoop/etc/hadoop文件夹中并没有mapred-site.xml文件，但提供了模板mapred-site.xml.template，将其复制一份重命名为mapred-site.xml 即可12[hadoop@node200 hadoop]$ cp mapred-site.xml.template mapred-site.xml[hadoop@node200 hadoop]$ vi mapred-site.xml 配置文件如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt; &lt;!-- Put site-specific property overrides in this file. --&gt; &lt;configuration&gt; &lt;!-- 指定MapReduce计算框架使用YARN --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定jobhistory server的rpc地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;node200:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定jobhistory server的http地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node200:19888&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启uber模式（针对小作业的优化） --&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.ubertask.enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置启动uber模式的最大map数 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.ubertask.maxmaps&lt;/name&gt; &lt;value&gt;9&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置启动uber模式的最大reduce数 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.ubertask.maxreduces&lt;/name&gt; &lt;value&gt;5&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5、修改yarn-site.xml1[hadoop@master200 hadoop]$ vi yarn-site.xml 配置文件如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140&lt;?xml version="1.0"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt; &lt;configuration&gt; &lt;!-- Site specific YARN configuration properties --&gt; &lt;!-- NodeManager上运行的附属服务，需配置成mapreduce_shuffle才可运行MapReduce程序 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置Web Application Proxy安全代理（防止yarn被攻击） --&gt; &lt;property&gt; &lt;name&gt;yarn.web-proxy.address&lt;/name&gt; &lt;value&gt;node201:8888&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启日志 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置日志删除时间为7天，-1为禁用，单位为秒 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt; &lt;/property&gt; &lt;!-- 修改日志目录 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt; &lt;value&gt;/usr/local/tmp/hadooplogs&lt;/value&gt; &lt;/property&gt; &lt;!--配置nodemanager可用的资源内存 &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; 配置nodemanager可用的资源CPU &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; --&gt; &lt;!-- //////////////以下为YARN HA的配置////////////// --&gt; &lt;!-- 开启YARN HA --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 启用自动故障转移 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定YARN HA的名称 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yarncluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定两个resourcemanager的名称 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置rm1，rm2的主机 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;node201&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;node200&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置YARN的http端口 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;node201:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;node200:8088&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置zookeeper的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;node202:2181,node203:2181,node204:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置zookeeper的存储位置 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-state-store.parent-path&lt;/name&gt; &lt;value&gt;/usr/local/zookeeperdata/rmstore&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启yarn resourcemanager restart --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置resourcemanager的状态存储到zookeeper中 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启yarn nodemanager restart --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置nodemanager IPC的通信端口 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.address&lt;/name&gt; &lt;value&gt;0.0.0.0:45454&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 6、修改slaves文件1[hadoop@node200 hadoop]$ vi slaves 配置文件如下：123node202node203node204 上述关键的配置文件我已经上传至：https://github.com/PengShuaixin/hadoop-2.7.3_centos7 可以直接下载下来，通过上传到Linux直接覆盖原来文件的方式进行配置 7、通过scp将配置文件上传到其他主机1234[hadoop@node200 hadoop]$ scp -r /usr/local/hadoop/etc/hadoop/* node201:/usr/local/hadoop/etc/hadoop[hadoop@node200 hadoop]$ scp -r /usr/local/hadoop/etc/hadoop/* node202:/usr/local/hadoop/etc/hadoop[hadoop@node200 hadoop]$ scp -r /usr/local/hadoop/etc/hadoop/* node203:/usr/local/hadoop/etc/hadoop[hadoop@node200 hadoop]$ scp -r /usr/local/hadoop/etc/hadoop/* node204:/usr/local/hadoop/etc/hadoop Hadoop集群的初始化1、启动zookeeper集群（分别在node202、node203和node204上执行）1zkServer.sh start ps：zookeeper其他命令1234#查看状态zkServer.sh status#关闭zkServer.sh stop 启动后查看状态如下：1234567891011121314151617181920#node202[hadoop@node202 ~]$ zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo.cfgMode: follower#node203[hadoop@node203 ~]$ zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo.cfgMode: leader#node204[hadoop@node204 ~]$ zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo.cfgMode: follower#只有一台主机为leader，剩下的都是follower#zookeeper配置的主机数一般为2n+1台，且最少需要3台 2、格式化ZKFC（在node200上执行）1[hadoop@node200 ~]$ hdfs zkfc -formatZK 出现如下代码说明执行成功：12345678910111213141516171819202122232418/10/12 09:19:11 INFO tools.DFSZKFailoverController: Failover controller configured for NameNode NameNode at node200/192.168.33.200:900018/10/12 09:19:12 INFO zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT18/10/12 09:19:12 INFO zookeeper.ZooKeeper: Client environment:host.name=node20018/10/12 09:19:12 INFO zookeeper.ZooKeeper: Client environment:java.version=1.8.0_18118/10/12 09:19:12 INFO zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation18/10/12 09:19:12 INFO zookeeper.ZooKeeper: Client environment:java.home=/usr/java/jdk1.8.0_181-amd64/jre18/10/12 09:19:12 INFO zookeeper.ZooKeeper: Client environment:java.class.path=/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar18/10/12 09:19:12 INFO zookeeper.ZooKeeper: Client environment:java.library.path=/usr/local/hadoop/lib/native18/10/12 09:19:12 INFO zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp18/10/12 09:19:12 INFO zookeeper.ZooKeeper: Client environment:java.compiler=&lt;NA&gt;18/10/12 09:19:12 INFO zookeeper.ZooKeeper: Client environment:os.name=Linux18/10/12 09:19:12 INFO zookeeper.ZooKeeper: Client environment:os.arch=amd6418/10/12 09:19:12 INFO zookeeper.ZooKeeper: Client environment:os.version=3.10.0-862.el7.x86_6418/10/12 09:19:12 INFO zookeeper.ZooKeeper: Client environment:user.name=hadoop18/10/12 09:19:12 INFO zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop18/10/12 09:19:12 INFO zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop18/10/12 09:19:12 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=node202:2181,node203:2181,node204:2181 sessionTimeout=5000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@57a3af2518/10/12 09:19:12 INFO zookeeper.ClientCnxn: Opening socket connection to server node202/192.168.33.202:2181. Will not attempt to authenticate using SASL (unknown error)18/10/12 09:19:12 INFO zookeeper.ClientCnxn: Socket connection established to node202/192.168.33.202:2181, initiating session18/10/12 09:19:12 INFO zookeeper.ClientCnxn: Session establishment complete on server node202/192.168.33.202:2181, sessionid = 0x16665c138780001, negotiated timeout = 500018/10/12 09:19:12 INFO ha.ActiveStandbyElector: Successfully created /hadoop-ha/mycluster in ZK.18/10/12 09:19:12 INFO zookeeper.ZooKeeper: Session: 0x16665c138780001 closed18/10/12 09:19:12 WARN ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x16665c13878000118/10/12 09:19:12 INFO zookeeper.ClientCnxn: EventThread shut down 3、启动journalnode（分别在node202、node203和node204上执行）1hadoop-daemon.sh start journalnode 执行过程如下：1234567891011121314151617181920212223[hadoop@node202 ~]$ hadoop-daemon.sh start journalnodestarting journalnode, logging to /usr/local/hadoop/logs/hadoop-hadoop-journalnode-node202.out[hadoop@node202 ~]$ jps1508 QuorumPeerMain3239 Jps3182 JournalNode [hadoop@node203 ~]$ hadoop-daemon.sh start journalnodestarting journalnode, logging to /usr/local/hadoop/logs/hadoop-hadoop-journalnode-node203.out[hadoop@node203 ~]$ jps3289 Jps1484 QuorumPeerMain3231 JournalNode [hadoop@node204 ~]$ hadoop-daemon.sh start journalnodestarting journalnode, logging to /usr/local/hadoop/logs/hadoop-hadoop-journalnode-node204.out[hadoop@node204 ~]$ jps3296 Jps1490 QuorumPeerMain3237 JournalNode #这里用jps命令确认JournalNode进程启动情况#QuorumPeerMain是第1步操作启动的zookeeper的进程 4、格式化HDFS（在node200上执行）1[hadoop@node200 ~]$ hdfs namenode -format 执行出现如下代码说明执行成功：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374[hadoop@node200 ~]$ hdfs namenode -format18/10/12 09:36:22 INFO namenode.NameNode: STARTUP_MSG: /************************************************************STARTUP_MSG: Starting NameNodeSTARTUP_MSG: host = node200/192.168.33.200STARTUP_MSG: args = [-format]STARTUP_MSG: version = 2.7.3STARTUP_MSG: classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jarSTARTUP_MSG: build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41ZSTARTUP_MSG: java = 1.8.0_181************************************************************/18/10/12 09:36:22 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]18/10/12 09:36:22 INFO namenode.NameNode: createNameNode [-format]18/10/12 09:36:23 WARN common.Util: Path /usr/local/hadoopdata/namenode should be specified as a URI in configuration files. Please update hdfs configuration.18/10/12 09:36:23 WARN common.Util: Path /usr/local/hadoopdata/namenode should be specified as a URI in configuration files. Please update hdfs configuration.Formatting using clusterid: CID-882887d1-b39c-419c-8945-f9c753f516ae18/10/12 09:36:23 INFO namenode.FSNamesystem: No KeyProvider found.18/10/12 09:36:23 INFO namenode.FSNamesystem: fsLock is fair:true18/10/12 09:36:23 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=100018/10/12 09:36:23 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true18/10/12 09:36:23 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.00018/10/12 09:36:23 INFO blockmanagement.BlockManager: The block deletion will start around 2018 十月 12 09:36:2318/10/12 09:36:23 INFO util.GSet: Computing capacity for map BlocksMap18/10/12 09:36:23 INFO util.GSet: VM type = 64-bit18/10/12 09:36:23 INFO util.GSet: 2.0% max memory 966.7 MB = 19.3 MB18/10/12 09:36:23 INFO util.GSet: capacity = 2^21 = 2097152 entries18/10/12 09:36:23 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false18/10/12 09:36:23 INFO blockmanagement.BlockManager: defaultReplication = 318/10/12 09:36:23 INFO blockmanagement.BlockManager: maxReplication = 51218/10/12 09:36:23 INFO blockmanagement.BlockManager: minReplication = 118/10/12 09:36:23 INFO blockmanagement.BlockManager: maxReplicationStreams = 218/10/12 09:36:23 INFO blockmanagement.BlockManager: replicationRecheckInterval = 300018/10/12 09:36:23 INFO blockmanagement.BlockManager: encryptDataTransfer = false18/10/12 09:36:23 INFO blockmanagement.BlockManager: maxNumBlocksToLog = 100018/10/12 09:36:23 INFO namenode.FSNamesystem: fsOwner = hadoop (auth:SIMPLE)18/10/12 09:36:23 INFO namenode.FSNamesystem: supergroup = supergroup18/10/12 09:36:23 INFO namenode.FSNamesystem: isPermissionEnabled = false18/10/12 09:36:23 INFO namenode.FSNamesystem: Determined nameservice ID: mycluster18/10/12 09:36:23 INFO namenode.FSNamesystem: HA Enabled: true18/10/12 09:36:23 INFO namenode.FSNamesystem: Append Enabled: true18/10/12 09:36:23 INFO util.GSet: Computing capacity for map INodeMap18/10/12 09:36:23 INFO util.GSet: VM type = 64-bit18/10/12 09:36:23 INFO util.GSet: 1.0% max memory 966.7 MB = 9.7 MB18/10/12 09:36:23 INFO util.GSet: capacity = 2^20 = 1048576 entries18/10/12 09:36:23 INFO namenode.FSDirectory: ACLs enabled? false18/10/12 09:36:23 INFO namenode.FSDirectory: XAttrs enabled? true18/10/12 09:36:23 INFO namenode.FSDirectory: Maximum size of an xattr: 1638418/10/12 09:36:23 INFO namenode.NameNode: Caching file names occuring more than 10 times18/10/12 09:36:23 INFO util.GSet: Computing capacity for map cachedBlocks18/10/12 09:36:23 INFO util.GSet: VM type = 64-bit18/10/12 09:36:23 INFO util.GSet: 0.25% max memory 966.7 MB = 2.4 MB18/10/12 09:36:23 INFO util.GSet: capacity = 2^18 = 262144 entries18/10/12 09:36:23 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.999000012874603318/10/12 09:36:23 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 018/10/12 09:36:23 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension = 3000018/10/12 09:36:23 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 1018/10/12 09:36:23 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 1018/10/12 09:36:23 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,2518/10/12 09:36:23 INFO namenode.FSNamesystem: Retry cache on namenode is enabled18/10/12 09:36:23 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis18/10/12 09:36:23 INFO util.GSet: Computing capacity for map NameNodeRetryCache18/10/12 09:36:23 INFO util.GSet: VM type = 64-bit18/10/12 09:36:23 INFO util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB18/10/12 09:36:23 INFO util.GSet: capacity = 2^15 = 32768 entries18/10/12 09:36:25 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1162737530-192.168.33.200-153930818528718/10/12 09:36:25 INFO common.Storage: Storage directory /usr/local/hadoopdata/namenode has been successfully formatted.18/10/12 09:36:25 INFO namenode.FSImageFormatProtobuf: Saving image file /usr/local/hadoopdata/namenode/current/fsimage.ckpt_0000000000000000000 using no compression18/10/12 09:36:25 INFO namenode.FSImageFormatProtobuf: Image file /usr/local/hadoopdata/namenode/current/fsimage.ckpt_0000000000000000000 of size 352 bytes saved in 0 seconds.18/10/12 09:36:25 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 018/10/12 09:36:25 INFO util.ExitUtil: Exiting with status 018/10/12 09:36:25 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************SHUTDOWN_MSG: Shutting down NameNode at node200/192.168.33.200************************************************************/ 5、将格式化后node200节点hadoop工作目录中的元数据目录复制到node201节点1[hadoop@node200 ~]$ scp -r /usr/local/hadoopdata node201:/usr/local/ 6、初始化完毕后可关闭journalnode（分别在node202、node203和node204上执行）1hadoop-daemon.sh stop journalnode Hadoop集群的启动配置了好久，看到可以启动了是不是特别开心，下面就一步步启动我们的集群吧 启动步骤1、启动zookeeper集群（分别在node202、node203和node204上执行）1zkServer.sh start 在初始化过程中，如果启动了zookeeper没有关闭进程，在这里就不用重复启动了 2、启动HDFS（在node200上执行）1[hadoop@node200 ~]$ start-dfs.sh 此命令分别在node200、node201节点启动了NameNode和ZKFC，分别在node202、node203、node204节点启动了DataNode和JournalNode，如下所示。1234567891011121314[hadoop@node200 ~]$ start-dfs.shStarting namenodes on [node200 node201]node200: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-namenode-node200.outnode201: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-namenode-node201.outnode204: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hadoop-datanode-node204.outnode202: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hadoop-datanode-node202.outnode203: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hadoop-datanode-node203.outStarting journal nodes [node202 node203 node204]node202: starting journalnode, logging to /usr/local/hadoop/logs/hadoop-hadoop-journalnode-node202.outnode204: starting journalnode, logging to /usr/local/hadoop/logs/hadoop-hadoop-journalnode-node204.outnode203: starting journalnode, logging to /usr/local/hadoop/logs/hadoop-hadoop-journalnode-node203.outStarting ZK Failover Controllers on NN hosts [node200 node201]node200: starting zkfc, logging to /usr/local/hadoop/logs/hadoop-hadoop-zkfc-node200.outnode201: starting zkfc, logging to /usr/local/hadoop/logs/hadoop-hadoop-zkfc-node201.out 3、启动YARN（在node201上执行）1[hadoop@node201 ~]$ start-yarn.sh 此命令在node201节点启动了ResourceManager，分别在node202、node203、node204节点启动了NodeManager。123456[hadoop@node201 ~]$ start-yarn.shstarting yarn daemonsstarting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-resourcemanager-node201.outnode202: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-nodemanager-node202.outnode203: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-nodemanager-node203.outnode204: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-nodemanager-node204.out 4、启动YARN的另一个ResourceManager（在node200执行，用于容灾）1[hadoop@node200 ~]$ yarn-daemon.sh start resourcemanager 12#执行过程如下starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-resourcemanager-node200.out 5、启动YARN的安全代理（在node201执行）1[hadoop@node201 ~]$ yarn-daemon.sh start proxyserver 12#执行过程如下starting proxyserver, logging to /usr/local/hadoop/logs/yarn-hadoop-proxyserver-node201.out 备注：proxyserver充当防火墙的角色，可以提高访问集群的安全性 6、启动YARN的历史任务服务（在node200执行）12345678#方法一：[hadoop@node200 ~]$ mr-jobhistory-daemon.sh start historyserver #执行过程如下starting historyserver, logging to /usr/local/hadoop/logs/mapred-hadoop-historyserver-node200.out #方法二：[hadoop@node200 ~]$ yarn-daemon.sh start historyserver 备注：yarn-daemon.sh start historyserver已被弃用；CDH版本似乎有个问题，即mapred-site.xml配置的的mapreduce.jobhistory.address和mapreduce.jobhistory.webapp.address参数似乎不起作用，实际对应的端口号是10200和8188，而且部需要配置就可以在任意节点上开启历史任务服务。 集群进程查看123456[hadoop@node200 ~]$ jps5313 DFSZKFailoverController5842 ResourceManager7049 Jps5002 NameNode6733 JobHistoryServer 123456[hadoop@node201 ~]$ jps4726 NameNode4839 DFSZKFailoverController6794 Jps5181 ResourceManager6110 WebAppProxyServer 123456[hadoop@node202 ~]$ jps1508 QuorumPeerMain5111 NodeManager4696 DataNode4794 JournalNode6318 Jps 123456[hadoop@node203 ~]$ jps5080 NodeManager1484 QuorumPeerMain4668 DataNode4767 JournalNode6303 Jps 123456[hadoop@node204 ~]$ jps1490 QuorumPeerMain6307 Jps4647 DataNode5064 NodeManager4746 JournalNode Web界面截图HDFSnode200：http://192.168.33.200:50070，可看到NameNode为active状态 node201：http://192.168.33.201:50070，可看到NameNode为standby状态 YARNnode201:http://192.168.33.201:8088，可看到ResourceManager为active状态 node200:http://192.168.33.200:8088，此时ResourceManager为standby状态， 网页无法直接访问，会自动跳转到node201的页面 相关推荐在Windows中安装Hadoop（非虚拟机安装） CentOS 7部署Hadoop（单机版） CentOS 7部署Hadoop（伪分布式） CentOS 7部署Hadoop集群（完全分布式） 到这里就配置完整个集群啦，若在配置过程中遇到什么问题，欢迎在下方评论区留言一起讨论！]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Linux</tag>
        <tag>HA高可用集群</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7部署Hadoop集群（完全分布式）]]></title>
    <url>%2Fpost%2FCentOS-7%E9%83%A8%E7%BD%B2Hadoop%E9%9B%86%E7%BE%A4%EF%BC%88%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%EF%BC%89%2F</url>
    <content type="text"><![CDATA[测试环境Linux系统版本：CentOS 7 64位 Hadoop版本：hadoop-2.7.3 Java版本：jdk-8u181-linux-x64 集群服务器节点与进程Hadoop中的HDFS和YARN都是主从结构，主从结构中的主节点和从节点有多重概念方式： 主节点 从节点 master slave 管理者 工作者 leader followe Hadoop集群中各个角色的名称： 服务 主节点 从节点 HDFS NameNode DataNode YARN ResourceManager NodeManager HDFSNameNode：主Master，整个Hadoop集群只能有一个，管理HDFS文件系统的命名空间，维护元数据信息，管理副本的配置和信息（默认三个副本），处理客户端读写请求。 DataNode：Slave 工作节点，集群一般会启动多个，负责存储数据块和数据块校验，执行客户端的读写请求，通过心跳机制定期向NameNode汇报运行状态和本地所有块的列表信息，在集群启动时DataNode项NameNode提供存储Block块的列表信息。 YARNResourceManager：整个集群只有一个Master，Slave可以有多个，支持高可用，处理客户端Client请求，启动／管理／监控ApplicationMaster，监控NodeManager，资源的分配和调度。 NodeManager：每个节点只有一个，一般与Data Node部署在同一台机器上且一一对应，定时向Resource Manager汇报本机资源的使用状况，处理来自Resource Manager的作业请求，为作业分配Container，处理来自Application Master的请求，启动和停止Container 为增加集群的容灾性，将对SecondaryNameNode进行配置 SecondaryNameNode：备份所有数据分布情况，当Namenode服务器宕机（日常所说的死机）时，可通过该服务器来恢复数据。 集群部署规划 主机名称 IP地址 用户名称 进程 node101 192.168.33.101 hadoop NameNode、ResourceManager node102 192.168.33.102 hadoop DataNode、NodeManager、SecondaryNameNode node103 192.168.33.103 hadoop DataNode、NodeManager node104 192.168.33.104 hadoop DataNode、NodeManager 搭建Linux系统按如下方法部署四台主机，主机名与IP地址的对应关系见上文集群部署规划 VMware虚拟机安装Linux系统 配置完成之后各主机IP与主机名信息如下：12345678910111213[root@node101 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:81:55:83 brd ff:ff:ff:ff:ff:ff inet 192.168.33.101/24 brd 192.168.33.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe81:5583/64 scope link valid_lft forever preferred_lft forever 12345678910111213[root@node102 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:c2:52:70 brd ff:ff:ff:ff:ff:ff inet 192.168.33.102/24 brd 192.168.33.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fec2:5270/64 scope link valid_lft forever preferred_lft forever 12345678910111213[root@node103 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:9b:9e:e2 brd ff:ff:ff:ff:ff:ff inet 192.168.33.103/24 brd 192.168.33.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe9b:9ee2/64 scope link valid_lft forever preferred_lft forever 12345678910111213[root@node104 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:76:44:a4 brd ff:ff:ff:ff:ff:ff inet 192.168.33.104/24 brd 192.168.33.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe76:44a4/64 scope link valid_lft forever preferred_lft foreve 配置完成之后测试各主机网络互通情况，在每台主机上执行下面两条命令，运行过程中按Ctrl+C可以终止进程，下面就不贴测试效果了12ping 192.168.33.1ping www.baidu.com 配置Java环境为上面安装的系统配置Java环境变量，本文中就写关键配置步骤与执行命令了，想了解详细的配置过程可以查看：Linux系统下安装Java环境 为了方便，本文就直接使用rpm包安装了，/etc/profile文件暂时不进行配置，到后面配置hadoop单机版时再进行配置 [1-3]均使用root用户执行 1、将安装包jdk-8u181-linux-x64.rpm上传到/usr/local目录下 2、安装rpm包，先设置权限，然后执行rpm命令安装12chmod 755 /usr/local/jdk-8u181-linux-x64.rpmrpm -ivh /usr/local/jdk-8u181-linux-x64.rpm 3、校验安装情况1java -version 安装单机版Hadoop详细步骤查看：Hadoop部署（二）——Linux系统下安装Java环境，这里只简单介绍安装步骤 [1-5]均使用root用户执行 1、将压缩包hadoop-2.7.3.tar.gz上传到/usr/local目录下 2、解压压缩包，进入/usr/local目录，对文件夹重命名123tar -zxvf /usr/local/hadoop-2.7.3.tar.gzcd /usr/localmv hadoop-2.7.3 hadoop 3、创建hadoop用户和hadoop用户组，并设置hadoop用户密码12useradd hadooppasswd hadoop 4、为hadoop用户添加sudo权限1vi /etc/sudoers 在root用户下面一行加上hadoop ALL=(ALL) ALL，保存并退出（这里需要用wq!强制保存退出）123456789101112## Next comes the main part: which users can run what software on## which machines (the sudoers file can be shared between multiple## systems).## Syntax:#### user MACHINE=COMMANDS#### The COMMANDS section may have other options added to it.#### Allow root to run any commands anywhereroot ALL=(ALL) ALLhadoop ALL=(ALL) ALL 5、将hadoop文件夹的主：组设置成hadoop，/usr目录与/usr/local目录所属主：组均为root，默认权限为755，也就是说其他用户（hadoop）没有写入（w）权限，在这里我们需要将这两个目录其他用户的权限设置为7123chown -R hadoop:hadoop hadoopchmod 757 /usrchmod 757 /usr/local 配置环境变量[1-3]均使用root用户执行 1、编辑/etc/profile文件1vi /etc/profile 2、在末尾加上如下几行代码12345export JAVA_HOME=/usr/java/jdk1.8.0_181-amd64export HADOOP_HOME=/usr/local/hadoopexport PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JRE_HOME=$JAVA_HOME/jre 3、配置完环境变量之后保存退出，让环境变量立即生效1source /etc/profile 关闭防火墙CentOS 7使用的是firewalld作为防火墙，与CentOS 6 有所不同 下面三步均使用root用户执行 查看防火墙状态：1systemctl status firewalld 关闭防火墙：1systemctl stop firewalld 关闭防火墙开机自动启动：1systemctl disable firewalld 更多详情可以了解：CentOS 7部署Hadoop（伪分布式） 修改hosts文件修改所有主机的/etc/hosts文件，这里使用root用户操作1vi /etc/hosts 在文件后面加上1234192.168.33.101 node101192.168.33.102 node102192.168.33.103 node103192.168.33.104 node104 注意：此处IP地址后面为Tab制表符，而不是空格 配置SSH免密登录所有步骤均使用hadoop用户进行操作，方法参照： Linux系统配置SSH免密登录(多主机互通)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#在每台主机上执行ssh-keygen -t rsa[hadoop@node101 ~]$ ssh-keygen -t rsa[hadoop@node102 ~]$ ssh-keygen -t rsa[hadoop@node103 ~]$ ssh-keygen -t rsa[hadoop@node104 ~]$ ssh-keygen -t rsa #生成authorized_keys[hadoop@node101 ~]$ ssh-copy-id localhost #将node101上的文件通过scp复制到其他主机，覆盖其他主机的密钥文件 [hadoop@node101 ~]$ scp -r ~/.ssh/* 192.168.33.102:~/.ssh The authenticity of host '192.168.33.102 (192.168.33.102)' can't be established.ECDSA key fingerprint is SHA256:mLD6JLZCaaM/4LNX5yw9zIpL0aJaiPLdcKau6gPJEzI.ECDSA key fingerprint is MD5:b5:ff:b7:d9:f7:76:77:57:df:a5:89:e9:63:63:d8:71.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added '192.168.33.102' (ECDSA) to the list of known hosts.hadoop@192.168.33.102's password: scp: /home/hadoop/.ssh: No such file or directory[hadoop@node101 ~]$ scp -r ~/.ssh/* 192.168.33.102:~/.sshhadoop@192.168.33.102's password: authorized_keys 100% 396 326.1KB/s 00:00 id_rsa 100% 1675 1.4MB/s 00:00 id_rsa.pub 100% 396 508.2KB/s 00:00 known_hosts 100% 347 594.5KB/s 00:00 [hadoop@node101 ~]$ scp -r ~/.ssh/* 192.168.33.103:~/.sshThe authenticity of host '192.168.33.103 (192.168.33.103)' can't be established.ECDSA key fingerprint is SHA256:8jtDe6bMz1Ej/L00sRkVp2P9GEUUBGBYHChtbDQIVkE.ECDSA key fingerprint is MD5:97:9f:d6:5e:c1:88:7f:b6:f2:df:3b:d7:cb:27:c9:f3.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added '192.168.33.103' (ECDSA) to the list of known hosts.hadoop@192.168.33.103's password: authorized_keys 100% 396 415.9KB/s 00:00 id_rsa 100% 1675 1.6MB/s 00:00 id_rsa.pub 100% 396 514.7KB/s 00:00 known_hosts 100% 523 554.1KB/s 00:00 [hadoop@node101 ~]$ scp -r ~/.ssh/* 192.168.33.104:~/.sshThe authenticity of host '192.168.33.104 (192.168.33.104)' can't be established.ECDSA key fingerprint is SHA256:J2aFGIz5bWg1IirGYnQrhBDAuXvSUB9qJyLcxyB+CQ4.ECDSA key fingerprint is MD5:eb:1e:e6:af:af:5a:11:92:e6:55:ff:a3:09:16:55:99.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added '192.168.33.104' (ECDSA) to the list of known hosts.hadoop@192.168.33.104's password: authorized_keys 100% 396 349.5KB/s 00:00 id_rsa 100% 1675 1.9MB/s 00:00 id_rsa.pub 100% 396 675.8KB/s 00:00 known_hosts 100% 699 453.9KB/s 00:00 修改Hadoop配置文件均使用hadoop用户操作，只需要在node101上修改即可 先进入/usr/local/hadoop/etc/hadoop/文件1[hadoop@node101 ~]$ cd /usr/local/hadoop/etc/hadoop/ 1、修改hadoop-env.sh文件1[hadoop@node101 hadoop]$ vi hadoop-env.sh 找到export JAVA_HOME=${JAVA_HOME}，在前面加个#注释掉，将JAVA_HOME用路径代替，如下：12#export JAVA_HOME=$&#123;JAVA_HOME&#125;export JAVA_HOME=/usr/java/jdk1.8.0_181-amd64 2、修改core-site.xml文件1[hadoop@master100 hadoop]$ vi core-site.xml 配置文件如下：123456789101112131415161718192021222324252627282930&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt; &lt;!-- Put site-specific property overrides in this file. --&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;指定hadoop运行时产生文件的存储路径&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node101:9000&lt;/value&gt; &lt;description&gt;hdfs namenode的通信地址,通信端口&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 3、修改hdfs-site.xml1[hadoop@node101 hadoop]$ vi hdfs-site.xml 配置文件如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt; &lt;!-- Put site-specific property overrides in this file. --&gt;&lt;!-- 该文件指定与HDFS相关的配置信息。需要修改HDFS默认的块的副本属性，因为HDFS默认情况下每个数据块保存3个副本，而在伪分布式模式下运行时，由于只有一个数据节点，所以需要将副本个数改为1；否则Hadoop程序会报错。 --&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;description&gt;指定HDFS存储数据的副本数目，默认情况下是3份&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hadoopdata/namenode&lt;/value&gt; &lt;description&gt;namenode存放数据的目录&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hadoopdata/datanode&lt;/value&gt; &lt;description&gt;datanode存放block块的目录&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.secondary.http.address&lt;/name&gt; &lt;value&gt;node102:50090&lt;/value&gt; &lt;description&gt;secondarynamenode 运行节点的信息，和 namenode 不同节点&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;关闭权限验证&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 4、修改mapred-site.xml /usr/local/hadoop/etc/hadoop文件夹中并没有mapred-site.xml文件，但提供了模板mapred-site.xml.template，将其复制一份重命名为mapred-site.xml 即可12[hadoop@node101 hadoop]$ cp mapred-site.xml.template mapred-site.xml[hadoop@node101 hadoop]$ vi mapred-site.xml 配置文件如下：1234567891011121314151617181920212223242526&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt; &lt;!-- Put site-specific property overrides in this file. --&gt;&lt;!-- 在该配置文件中指定与MapReduce作业相关的配置属性，需要指定JobTracker运行的主机地址--&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;description&gt;指定mapreduce运行在yarn上&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 5、修改yarn-site.xml1[hadoop@master100 hadoop]$ vi yarn-site.xml 配置文件如下：123456789101112131415161718192021222324252627&lt;?xml version="1.0"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;configuration&gt; &lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node101&lt;/value&gt; &lt;description&gt;yarn总管理器的IPC通讯地址&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;description&gt;mapreduce执行shuffle时获取数据的方式&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 6、修改slaves文件1[hadoop@node101 hadoop]$ vi slaves 配置文件如下：123node102node103node104 上述配置文件我已经上传至：https://github.com/PengShuaixin/hadoop-2.7.3_centos7 可以直接下载下来，通过上传到Linux直接覆盖原来文件的方式进行配置 7、通过scp将配置文件上传到其他主机12345[hadoop@node101 hadoop]$ scp -r /usr/local/hadoop/etc/hadoop/* node102:/usr/local/hadoop/etc/hadoop [hadoop@node101 hadoop]$ scp -r /usr/local/hadoop/etc/hadoop/* node103:/usr/local/hadoop/etc/hadoop [hadoop@node101 hadoop]$ scp -r /usr/local/hadoop/etc/hadoop/* node104:/usr/local/hadoop/etc/hadoop HDFS初始化Hadoop配置完后，用hadoop用户操作，在node101上格式化namenode1[hadoop@node101 hadoop]$ hdfs namenode -format 出现如下信息说明格式化成功：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071[hadoop@node101 hadoop]$ hdfs namenode -format18/10/04 16:21:27 INFO namenode.NameNode: STARTUP_MSG: /************************************************************STARTUP_MSG: Starting NameNodeSTARTUP_MSG: host = node101/192.168.33.101STARTUP_MSG: args = [-format]STARTUP_MSG: version = 2.7.3STARTUP_MSG: classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jarSTARTUP_MSG: build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41ZSTARTUP_MSG: java = 1.8.0_181************************************************************/18/10/04 16:21:27 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]18/10/04 16:21:27 INFO namenode.NameNode: createNameNode [-format]Formatting using clusterid: CID-f67c3bc3-0ce6-4def-8b59-64af51cb4f3518/10/04 16:21:28 INFO namenode.FSNamesystem: No KeyProvider found.18/10/04 16:21:28 INFO namenode.FSNamesystem: fsLock is fair:true18/10/04 16:21:28 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=100018/10/04 16:21:28 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true18/10/04 16:21:28 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.00018/10/04 16:21:28 INFO blockmanagement.BlockManager: The block deletion will start around 2018 十月 04 16:21:2818/10/04 16:21:28 INFO util.GSet: Computing capacity for map BlocksMap18/10/04 16:21:28 INFO util.GSet: VM type = 64-bit18/10/04 16:21:28 INFO util.GSet: 2.0% max memory 966.7 MB = 19.3 MB18/10/04 16:21:28 INFO util.GSet: capacity = 2^21 = 2097152 entries18/10/04 16:21:28 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false18/10/04 16:21:28 INFO blockmanagement.BlockManager: defaultReplication = 318/10/04 16:21:28 INFO blockmanagement.BlockManager: maxReplication = 51218/10/04 16:21:28 INFO blockmanagement.BlockManager: minReplication = 118/10/04 16:21:28 INFO blockmanagement.BlockManager: maxReplicationStreams = 218/10/04 16:21:28 INFO blockmanagement.BlockManager: replicationRecheckInterval = 300018/10/04 16:21:28 INFO blockmanagement.BlockManager: encryptDataTransfer = false18/10/04 16:21:28 INFO blockmanagement.BlockManager: maxNumBlocksToLog = 100018/10/04 16:21:28 INFO namenode.FSNamesystem: fsOwner = hadoop (auth:SIMPLE)18/10/04 16:21:28 INFO namenode.FSNamesystem: supergroup = supergroup18/10/04 16:21:28 INFO namenode.FSNamesystem: isPermissionEnabled = false18/10/04 16:21:28 INFO namenode.FSNamesystem: HA Enabled: false18/10/04 16:21:28 INFO namenode.FSNamesystem: Append Enabled: true18/10/04 16:21:29 INFO util.GSet: Computing capacity for map INodeMap18/10/04 16:21:29 INFO util.GSet: VM type = 64-bit18/10/04 16:21:29 INFO util.GSet: 1.0% max memory 966.7 MB = 9.7 MB18/10/04 16:21:29 INFO util.GSet: capacity = 2^20 = 1048576 entries18/10/04 16:21:29 INFO namenode.FSDirectory: ACLs enabled? false18/10/04 16:21:29 INFO namenode.FSDirectory: XAttrs enabled? true18/10/04 16:21:29 INFO namenode.FSDirectory: Maximum size of an xattr: 1638418/10/04 16:21:29 INFO namenode.NameNode: Caching file names occuring more than 10 times18/10/04 16:21:29 INFO util.GSet: Computing capacity for map cachedBlocks18/10/04 16:21:29 INFO util.GSet: VM type = 64-bit18/10/04 16:21:29 INFO util.GSet: 0.25% max memory 966.7 MB = 2.4 MB18/10/04 16:21:29 INFO util.GSet: capacity = 2^18 = 262144 entries18/10/04 16:21:29 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.999000012874603318/10/04 16:21:29 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 018/10/04 16:21:29 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension = 3000018/10/04 16:21:29 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 1018/10/04 16:21:29 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 1018/10/04 16:21:29 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,2518/10/04 16:21:29 INFO namenode.FSNamesystem: Retry cache on namenode is enabled18/10/04 16:21:29 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis18/10/04 16:21:29 INFO util.GSet: Computing capacity for map NameNodeRetryCache18/10/04 16:21:29 INFO util.GSet: VM type = 64-bit18/10/04 16:21:29 INFO util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB18/10/04 16:21:29 INFO util.GSet: capacity = 2^15 = 32768 entries18/10/04 16:21:29 INFO namenode.FSImage: Allocated new BlockPoolId: BP-148426469-192.168.33.101-153864128943418/10/04 16:21:29 INFO common.Storage: Storage directory /usr/local/hadoop/hadoopdata/namenode has been successfully formatted.18/10/04 16:21:29 INFO namenode.FSImageFormatProtobuf: Saving image file /usr/local/hadoop/hadoopdata/namenode/current/fsimage.ckpt_0000000000000000000 using no compression18/10/04 16:21:29 INFO namenode.FSImageFormatProtobuf: Image file /usr/local/hadoop/hadoopdata/namenode/current/fsimage.ckpt_0000000000000000000 of size 353 bytes saved in 0 seconds.18/10/04 16:21:29 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 018/10/04 16:21:29 INFO util.ExitUtil: Exiting with status 018/10/04 16:21:29 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************SHUTDOWN_MSG: Shutting down NameNode at node101/192.168.33.101************************************************************/ 启动Hadoop使用hadoop用户操作 启动HDFS注意：不管在集群中的那个节点都可以1[hadoop@node101 hadoop]$ start-dfs.sh 可看到如下信息：12345678910111213[hadoop@node101 hadoop]$ start-dfs.shStarting namenodes on [node101]The authenticity of host 'node101 (192.168.33.101)' can't be established.ECDSA key fingerprint is SHA256:jXz9wiErwjKiKaa+PJoCRyecdM3jVnu+AW2PrZucWxk.ECDSA key fingerprint is MD5:d1:a2:b4:6d:30:21:d7:f8:3c:17:e8:43:93:6c:5e:da.Are you sure you want to continue connecting (yes/no)? yesnode101: Warning: Permanently added 'node101,192.168.33.101' (ECDSA) to the list of known hosts.node101: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-namenode-node101.outnode103: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hadoop-datanode-node103.outnode104: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hadoop-datanode-node104.outnode102: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hadoop-datanode-node102.outStarting secondary namenodes [node102]node102: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-secondarynamenode-node102.out 启动YARN注意：只能在主节点中进行启动 1[hadoop@node101 hadoop]$ start-yarn.sh 可看到如下信息：123456[hadoop@node101 hadoop]$ start-yarn.shstarting yarn daemonsstarting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-resourcemanager-node101.outnode104: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-nodemanager-node104.outnode102: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-nodemanager-node102.outnode103: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-nodemanager-node103.out 查看服务器进程1jps node1011234[hadoop@node101 hadoop]$ jps15713 Jps14847 NameNode15247 ResourceManager node10212345[hadoop@node102 ~]$ jps15228 NodeManager14925 DataNode14989 SecondaryNameNode15567 Jps node1031234[hadoop@node103 ~]$ jps14532 DataNode15143 Jps14766 NodeManager node1041234[hadoop@node104 ~]$ jps15217 Jps14805 NodeManager14571 DataNode 可以看到与我们集群规划所分配的进程是一致的 启动HDFS和YARN的web管理界面HDFS : http://192.168.33.101:50070 YARN : http://192.168.33.101:8088 HDFS界面 YARN界面 相关推荐在Windows中安装Hadoop（非虚拟机安装） CentOS 7部署Hadoop（单机版） CentOS 7部署Hadoop（伪分布式） CentOS 7部署Hadoop集群（HA高可用集群）]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Linux</tag>
        <tag>Hadoop集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7部署Hadoop（伪分布式）]]></title>
    <url>%2Fpost%2FCentOS-7%E9%83%A8%E7%BD%B2Hadoop%EF%BC%88%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%EF%BC%89%2F</url>
    <content type="text"><![CDATA[测试环境Linux系统版本：CentOS 7 64位 Hadoop版本：hadoop-2.7.3 Java版本：jdk-8u181-linux-x64 安装CentOS 7VMware虚拟机安装Linux系统 配置Java环境Linux系统下安装Java环境 配置单机版HadoopCentOS 7部署Hadoop（单机版） 配置SSH免密登录用hadoop用户登录进行下面的操作： 1、生成公钥私钥对，输入下面命令一直回车就可以了1ssh-keygen -t rsa 执行情况如下：123456789101112131415161718192021[hadoop@master100 ~]$ ssh-keygen -t rsaGenerating public/private rsa key pair.Enter file in which to save the key (/home/hadoop/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/hadoop/.ssh/id_rsa.Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.The key fingerprint is:SHA256:nx33fWWuccTRMo+zMzr/v4bMenWYFoPuFmOucqdwp90 hadoop@master100The key's randomart image is:+---[RSA 2048]----+| || .|| .o..|| . o*.|| S .. +=*|| . o=o+O+|| . ++=+B B|| .o.+*B B.|| o+*=oEo=|+----[SHA256]-----+ 2、实现本地免密登录,将id_rsa.pub中的内容拷贝到authorized_keys1ssh-copy-id localhost 12345678[hadoop@master100 ~]$ ssh-copy-id localhost/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/home/hadoop/.ssh/id_rsa.pub"/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keyshadoop@localhost's password: //这里输入hadoop用户的密码Number of key(s) added: 1Now try logging into the machine, with: "ssh 'localhost'"and check to make sure that only the key(s) you wanted were added. ~/.ssh目录下会生成一个新的文件:authorized_keys，如下：1234567[hadoop@master100 ~]$ cd ~/.ssh[hadoop@master100 .ssh]$ ll总用量 16-rw-------. 1 hadoop hadoop 398 10月 3 19:47 authorized_keys-rw-------. 1 hadoop hadoop 1679 10月 3 19:46 id_rsa-rw-r--r--. 1 hadoop hadoop 398 10月 3 19:46 id_rsa.pub-rw-r--r--. 1 hadoop hadoop 171 10月 3 19:42 known_hosts 3、完成上述步骤后就可以本地SSH免密登录了,运行下面代码出现一行登录时间就代表本地SSH免密登录成功1ssh localhost 下面是本地SSH免密登录成功的标志:12[hadoop@master100 ~]$ ssh localhostLast login: Wed Oct 3 19:45:11 2018 from 192.168.33.2 想了解更多，可以参考下文或者是自己百度相关知识 Linux系统配置SSH免密登录(多主机互通) 修改Hadoop配置文件1、进入/usr/local/hadoop/etc/hadoop/目录，可以看到如下配置文件，如果需要修改的文件不存在，那就创建一个，vi命令可以直接创建1[hadoop@master100 ~]$ cd /usr/local/hadoop/etc/hadoop/ 12345678910111213141516171819202122232425262728293031[hadoop@master100 hadoop]$ ll总用量 152-rw-r--r--. 1 hadoop hadoop 4436 8月 18 2016 capacity-scheduler.xml-rw-r--r--. 1 hadoop hadoop 1335 8月 18 2016 configuration.xsl-rw-r--r--. 1 hadoop hadoop 318 8月 18 2016 container-executor.cfg-rw-r--r--. 1 hadoop hadoop 774 8月 18 2016 core-site.xml-rw-r--r--. 1 hadoop hadoop 3589 8月 18 2016 hadoop-env.cmd-rw-r--r--. 1 hadoop hadoop 4224 8月 18 2016 hadoop-env.sh-rw-r--r--. 1 hadoop hadoop 2598 8月 18 2016 hadoop-metrics2.properties-rw-r--r--. 1 hadoop hadoop 2490 8月 18 2016 hadoop-metrics.properties-rw-r--r--. 1 hadoop hadoop 9683 8月 18 2016 hadoop-policy.xml-rw-r--r--. 1 hadoop hadoop 775 8月 18 2016 hdfs-site.xml-rw-r--r--. 1 hadoop hadoop 1449 8月 18 2016 httpfs-env.sh-rw-r--r--. 1 hadoop hadoop 1657 8月 18 2016 httpfs-log4j.properties-rw-r--r--. 1 hadoop hadoop 21 8月 18 2016 httpfs-signature.secret-rw-r--r--. 1 hadoop hadoop 620 8月 18 2016 httpfs-site.xml-rw-r--r--. 1 hadoop hadoop 3518 8月 18 2016 kms-acls.xml-rw-r--r--. 1 hadoop hadoop 1527 8月 18 2016 kms-env.sh-rw-r--r--. 1 hadoop hadoop 1631 8月 18 2016 kms-log4j.properties-rw-r--r--. 1 hadoop hadoop 5511 8月 18 2016 kms-site.xml-rw-r--r--. 1 hadoop hadoop 11237 8月 18 2016 log4j.properties-rw-r--r--. 1 hadoop hadoop 931 8月 18 2016 mapred-env.cmd-rw-r--r--. 1 hadoop hadoop 1383 8月 18 2016 mapred-env.sh-rw-r--r--. 1 hadoop hadoop 4113 8月 18 2016 mapred-queues.xml.template-rw-r--r--. 1 hadoop hadoop 758 8月 18 2016 mapred-site.xml.template-rw-r--r--. 1 hadoop hadoop 10 8月 18 2016 slaves-rw-r--r--. 1 hadoop hadoop 2316 8月 18 2016 ssl-client.xml.example-rw-r--r--. 1 hadoop hadoop 2268 8月 18 2016 ssl-server.xml.example-rw-r--r--. 1 hadoop hadoop 2191 8月 18 2016 yarn-env.cmd-rw-r--r--. 1 hadoop hadoop 4567 8月 18 2016 yarn-env.sh-rw-r--r--. 1 hadoop hadoop 690 8月 18 2016 yarn-site.xml 2、修改core-site.xml文件1[hadoop@master100 hadoop]$ vi core-site.xml 配置文件如下：123456789101112131415161718192021222324252627282930&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt; &lt;!-- Put site-specific property overrides in this file. --&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;指定hadoop运行时产生文件的存储路径&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;description&gt;hdfs namenode的通信地址,通信端口&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 3、修改hdfs-site.xml1vi hdfs-site.xml 配置文件如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt; &lt;!-- Put site-specific property overrides in this file. --&gt;&lt;!-- 该文件指定与HDFS相关的配置信息。需要修改HDFS默认的块的副本属性，因为HDFS默认情况下每个数据块保存3个副本，而在伪分布式模式下运行时，由于只有一个数据节点，所以需要将副本个数改为1；否则Hadoop程序会报错。 --&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;description&gt;指定HDFS存储数据的副本数目，默认情况下是3份&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hadoopdata/namenode&lt;/value&gt; &lt;description&gt;namenode存放数据的目录&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hadoopdata/datanode&lt;/value&gt; &lt;description&gt;datanode存放block块的目录&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;关闭权限验证&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 4、修改mapred-site.xml /usr/local/hadoop/etc/hadoop文件夹中并没有mapred-site.xml文件，但提供了模板mapred-site.xml.template，将其复制一份重命名为mapred-site.xml 即可123[hadoop@master100 hadoop]$ cp mapred-site.xml.template mapred-site.xml[hadoop@master100 hadoop]$ vi mapred-site.xml 配置文件如下：1234567891011121314151617181920212223242526&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt; &lt;!-- Put site-specific property overrides in this file. --&gt;&lt;!-- 在该配置文件中指定与MapReduce作业相关的配置属性，需要指定JobTracker运行的主机地址--&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;description&gt;指定mapreduce运行在yarn上&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 5、修改yarn-site.xml1[hadoop@master100 hadoop]$ vi yarn-site.xml 1234567891011121314151617181920212223&lt;?xml version="1.0"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;configuration&gt; &lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;description&gt;mapreduce执行shuffle时获取数据的方式&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 6、为了防止运行时报错，修改一下hadoop-env.sh文件1[hadoop@master100 hadoop]$ vi hadoop-env.sh 找到export JAVA_HOME=${JAVA_HOME}，在前面加个#注释掉，将JAVA_HOME用路径代替，如下：12#export JAVA_HOME=$&#123;JAVA_HOME&#125;export JAVA_HOME=/usr/java/jdk1.8.0_181-amd64 上述配置文件我已经上传至：https://github.com/PengShuaixin/hadoop-2.7.3_centos7 可以直接下载下来，通过上传到Linux直接覆盖原来文件的方式进行配置 关闭防火墙CentOS 7 使用的是firewalld作为防火墙，与CentOS 6 有所不同 查看防火墙状态：1systemctl status firewalld 如下状态说明正在运行：12345678[hadoop@master100 hadoop]$ systemctl status firewalld● firewalld.service - firewalld - dynamic firewall daemon Loaded: loaded (/usr/lib/systemd/system/firewalld.service; enabled; vendor preset: enabled) Active: active (running) since 三 2018-10-03 19:35:22 CST; 1h 49min ago Docs: man:firewalld(1) Main PID: 724 (firewalld) CGroup: /system.slice/firewalld.service └─724 /usr/bin/python -Es /usr/sbin/firewalld --nofork --nopid 关闭防火墙：1systemctl stop firewalld 关闭之后状态如下：12345678910111213[hadoop@master100 hadoop]$ systemctl stop firewalld==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-units ===Authentication is required to manage system services or units.Authenticating as: rootPassword: #这里需要输入root账户密码==== AUTHENTICATION COMPLETE ===[hadoop@master100 hadoop]$ systemctl status firewalld● firewalld.service - firewalld - dynamic firewall daemon Loaded: loaded (/usr/lib/systemd/system/firewalld.service; enabled; vendor preset: enabled) Active: inactive (dead) since 三 2018-10-03 21:27:34 CST; 40s ago Docs: man:firewalld(1) Process: 724 ExecStart=/usr/sbin/firewalld --nofork --nopid $FIREWALLD_ARGS (code=exited, status=0/SUCCESS) Main PID: 724 (code=exited, status=0/SUCCESS) 关闭防火墙开机自动启动：1systemctl disable firewalld 执行过程如下：123456789101112131415161718[hadoop@master100 hadoop]$ systemctl disable firewalld ==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-unit-files ===Authentication is required to manage system service or unit files.Authenticating as: rootPassword: #输入root用户密码==== AUTHENTICATION COMPLETE ===Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.==== AUTHENTICATING FOR org.freedesktop.systemd1.reload-daemon ===Authentication is required to reload the systemd state.Authenticating as: rootPassword: #输入root用户密码==== AUTHENTICATION COMPLETE === #查看开机启动状态，确认关闭成功[hadoop@master100 hadoop]$ systemctl is-enabled firewalld.servicedisabled 更多firewalld防火墙操作请参考： CentOS7使用firewalld打开关闭防火墙与端口 HDFS初始化Hadoop配置完后，格式化namenode1[hadoop@master100 hadoop]$ hdfs namenode -format 出现如下信息说明格式化成功：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071 [hadoop@master100 hadoop]$ hdfs namenode -format18/10/03 21:05:52 INFO namenode.NameNode: STARTUP_MSG: /************************************************************STARTUP_MSG: Starting NameNodeSTARTUP_MSG: host = master100/192.168.33.100STARTUP_MSG: args = [-format]STARTUP_MSG: version = 2.7.3STARTUP_MSG: classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jarSTARTUP_MSG: build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41ZSTARTUP_MSG: java = 1.8.0_181************************************************************/18/10/03 21:05:52 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]18/10/03 21:05:52 INFO namenode.NameNode: createNameNode [-format]Formatting using clusterid: CID-54d602ae-8c60-49f4-bcb9-8a687ba9750118/10/03 21:05:52 INFO namenode.FSNamesystem: No KeyProvider found.18/10/03 21:05:52 INFO namenode.FSNamesystem: fsLock is fair:true18/10/03 21:05:52 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=100018/10/03 21:05:52 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true18/10/03 21:05:52 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.00018/10/03 21:05:52 INFO blockmanagement.BlockManager: The block deletion will start around 2018 十月 03 21:05:5218/10/03 21:05:52 INFO util.GSet: Computing capacity for map BlocksMap18/10/03 21:05:52 INFO util.GSet: VM type = 64-bit18/10/03 21:05:52 INFO util.GSet: 2.0% max memory 966.7 MB = 19.3 MB18/10/03 21:05:52 INFO util.GSet: capacity = 2^21 = 2097152 entries18/10/03 21:05:52 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false18/10/03 21:05:52 INFO blockmanagement.BlockManager: defaultReplication = 118/10/03 21:05:52 INFO blockmanagement.BlockManager: maxReplication = 51218/10/03 21:05:52 INFO blockmanagement.BlockManager: minReplication = 118/10/03 21:05:52 INFO blockmanagement.BlockManager: maxReplicationStreams = 218/10/03 21:05:52 INFO blockmanagement.BlockManager: replicationRecheckInterval = 300018/10/03 21:05:52 INFO blockmanagement.BlockManager: encryptDataTransfer = false18/10/03 21:05:52 INFO blockmanagement.BlockManager: maxNumBlocksToLog = 100018/10/03 21:05:52 INFO namenode.FSNamesystem: fsOwner = hadoop (auth:SIMPLE)18/10/03 21:05:52 INFO namenode.FSNamesystem: supergroup = supergroup18/10/03 21:05:52 INFO namenode.FSNamesystem: isPermissionEnabled = true18/10/03 21:05:52 INFO namenode.FSNamesystem: HA Enabled: false18/10/03 21:05:52 INFO namenode.FSNamesystem: Append Enabled: true18/10/03 21:05:52 INFO util.GSet: Computing capacity for map INodeMap18/10/03 21:05:52 INFO util.GSet: VM type = 64-bit18/10/03 21:05:52 INFO util.GSet: 1.0% max memory 966.7 MB = 9.7 MB18/10/03 21:05:52 INFO util.GSet: capacity = 2^20 = 1048576 entries18/10/03 21:05:52 INFO namenode.FSDirectory: ACLs enabled? false18/10/03 21:05:52 INFO namenode.FSDirectory: XAttrs enabled? true18/10/03 21:05:52 INFO namenode.FSDirectory: Maximum size of an xattr: 1638418/10/03 21:05:52 INFO namenode.NameNode: Caching file names occuring more than 10 times18/10/03 21:05:53 INFO util.GSet: Computing capacity for map cachedBlocks18/10/03 21:05:53 INFO util.GSet: VM type = 64-bit18/10/03 21:05:53 INFO util.GSet: 0.25% max memory 966.7 MB = 2.4 MB18/10/03 21:05:53 INFO util.GSet: capacity = 2^18 = 262144 entries18/10/03 21:05:53 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.999000012874603318/10/03 21:05:53 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 018/10/03 21:05:53 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension = 3000018/10/03 21:05:53 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 1018/10/03 21:05:53 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 1018/10/03 21:05:53 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,2518/10/03 21:05:53 INFO namenode.FSNamesystem: Retry cache on namenode is enabled18/10/03 21:05:53 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis18/10/03 21:05:53 INFO util.GSet: Computing capacity for map NameNodeRetryCache18/10/03 21:05:53 INFO util.GSet: VM type = 64-bit18/10/03 21:05:53 INFO util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB18/10/03 21:05:53 INFO util.GSet: capacity = 2^15 = 32768 entries18/10/03 21:05:53 INFO namenode.FSImage: Allocated new BlockPoolId: BP-938082284-192.168.33.100-153857195331118/10/03 21:05:53 INFO common.Storage: Storage directory /usr/local/hadoop/hadoopdata/namenode has been successfully formatted.18/10/03 21:05:53 INFO namenode.FSImageFormatProtobuf: Saving image file /usr/local/hadoop/hadoopdata/namenode/current/fsimage.ckpt_0000000000000000000 using no compression18/10/03 21:05:53 INFO namenode.FSImageFormatProtobuf: Image file /usr/local/hadoop/hadoopdata/namenode/current/fsimage.ckpt_0000000000000000000 of size 353 bytes saved in 0 seconds.18/10/03 21:05:53 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 018/10/03 21:05:53 INFO util.ExitUtil: Exiting with status 018/10/03 21:05:53 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************SHUTDOWN_MSG: Shutting down NameNode at master100/192.168.33.100************************************************************/ 启动Hadoop执行命令：12345start-all.sh #或者是使用如下两条命令启动hadoop,第一条命令实际上操作的就是下面两条命令start-dfs.shstart-yarn.sh 出现如下信息：123456789101112131415161718[hadoop@master100 hadoop]$ start-all.sh #执行上述命令会出现如下信息This script is Deprecated. Instead use start-dfs.sh and start-yarn.shStarting namenodes on [localhost]localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-namenode-master100.outlocalhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hadoop-datanode-master100.outStarting secondary namenodes [0.0.0.0]The authenticity of host '0.0.0.0 (0.0.0.0)' can't be established.ECDSA key fingerprint is SHA256:smnjN2nAXE9l/kx1fQ2r3KBQt14TMjgaVlh+65clSrA.ECDSA key fingerprint is MD5:75:f5:c8:d0:06:c2:c8:0d:25:8b:b9:d5:47:8c:3a:f5.#到这里时注意输入yes后回车Are you sure you want to continue connecting (yes/no)? yes0.0.0.0: Warning: Permanently added '0.0.0.0' (ECDSA) to the list of known hosts.0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-secondarynamenode-master100.outstarting yarn daemonsstarting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-resourcemanager-master100.outlocalhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-nodemanager-master100.out 查看启动的进程：1jps 出现如下进程说明Hadoop安装并启动成功1234567[hadoop@master100 hadoop]$ jps6566 NameNode7399 Jps6664 DataNode7016 ResourceManager7114 NodeManager6862 SecondaryNameNode 可以进入如下网页查看Namenode与ResourceManager192.168.33.100:50070 192.168.33.100:8088 关闭Hadoop123456789101112[hadoop@master100 sbin]$ stop-all.sh This script is Deprecated. Instead use stop-dfs.sh and stop-yarn.shStopping namenodes on [localhost]localhost: stopping namenodelocalhost: stopping datanodeStopping secondary namenodes [0.0.0.0]0.0.0.0: stopping secondarynamenodestopping yarn daemonsstopping resourcemanagerlocalhost: stopping nodemanagerno proxyserver to stop 相关推荐在Windows中安装Hadoop（非虚拟机安装） CentOS 7部署Hadoop（单机版） CentOS 7部署Hadoop集群（完全分布式） CentOS 7部署Hadoop集群（HA高可用集群）]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>CentOS</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7部署Hadoop（单机版）]]></title>
    <url>%2Fpost%2FCentOS-7%E9%83%A8%E7%BD%B2Hadoop%EF%BC%88%E5%8D%95%E6%9C%BA%E7%89%88%EF%BC%89%2F</url>
    <content type="text"><![CDATA[测试环境Linux系统版本：CentOS 7 64位 Hadoop版本：hadoop-2.7.3 Java版本：jdk-8u181-linux-x64 Hadoop部署方式介绍Hadoop部署方式分三种：Standalone Mode（单机模式）、Pseudo-Distributed Mode（伪分布式模式）、Fully Distributed Mode（全分布式模式） 单机模式单机模式是Hadoop的默认模式。这种模式在一台单机上运行，没有分布式文件系统，而是直接读写本地操作系统的文件系统。当首次解压Hadoop的源码包时，Hadoop无法了解硬件安装环境，便保守地选择了最小配置。在这种默认模式下所有3个XML文件均为空。当配置文件为空时，Hadoop会完全运行在本地。因为不需要与其他节点交互，单机模式就不使用HDFS，也不加载任何Hadoop的守护进程。该模式主要用于开发调试MapReduce程序的应用逻辑。 伪分布模式这种模式也是在一台单机上运行，但用不同的Java进程模仿分布式运行中的各类结点伪分布模式在“单节点集群”上运行Hadoop，其中所有的守护进程都运行在同一台机器上。该模式在单机模式之上增加了代码调试功能，允许你检查内存使用情况，HDFS输入输出，以及其他的守护进程。 全分布模式Hadoop守护进程运行在一个集群上。 部分内容来自：https://blog.csdn.net/bloodlc/article/details/19973009?utm_source=copy 安装CentOS 7VMware虚拟机安装Linux系统 配置Java环境Linux系统下安装Java环境 Hadoop单机版安装本文的代码框解释：#后面的为执行代码，[]内root为登录用户，@后master100表示主机名，~代表当前目录，按下面步骤执行时要注意自己这些信息和我的是不是一样的 1、下载Hadoop2.7.31hadoop-2.7.3.tar.gz 1https://archive.apache.org/dist/hadoop/common/hadoop-2.7.3/ 2、用root账户登录，创建hadoop用户和hadoop用户组，创建用户的时候会自动创建相应的用户组1[root@master100 ~]# useradd hadoop 3、创建完用户之后设置一个用户密码，有些密码太简单会提示无效的密码，提示重新输入时再输一遍就可以强制保存了123456[root@master100 ~]# passwd hadoop更改用户 hadoop 的密码 。新的 密码：无效的密码： 密码少于 8 个字符重新输入新的 密码：passwd：所有的身份验证令牌已经成功更新。 4、为hadoop用户添加sudo权限1[root@master100 ~]# vi /etc/sudoers 如下，在root用户下面一行加上hadoop ALL=(ALL) ALL，保存并退出（这里需要用wq!强制保存退出）123456789101112## Next comes the main part: which users can run what software on## which machines (the sudoers file can be shared between multiple## systems).## Syntax:#### user MACHINE=COMMANDS#### The COMMANDS section may have other options added to it.#### Allow root to run any commands anywhereroot ALL=(ALL) ALLhadoop ALL=(ALL) ALL 5、上传第1步下载好的hadoop-2.7.3.tar.gz 包到Linux系统中的/usr/local目录下1234sftp:/root&gt; cd /usr/localsftp:/usr/local&gt; Uploading hadoop-2.7.3.tar.gz to remote:/usr/local/hadoop-2.7.3.tar.gzsftp: sent 204 MB in 9.38 seconds 6、进入/usr/local目录，解压上传的hadoop安装包12[root@master100 ~]# cd /usr/local[root@master100 local]# tar -zxvf /usr/local/hadoop-2.7.3.tar.gz 7、解压完成后可以看到文件夹hadoop-2.7.3，将文件夹名改为hadoop，如果不在/usr/local目录下，命令使用时请加上文件夹的绝对路径12345678910111213141516[root@master100 local]# ll总用量 556412drwxr-xr-x. 2 root root 6 4月 11 12:59 bindrwxr-xr-x. 2 root root 6 4月 11 12:59 etcdrwxr-xr-x. 2 root root 6 4月 11 12:59 gamesdrwxr-xr-x. 9 root root 149 8月 18 2016 hadoop-2.7.3-rw-r--r--. 1 root root 214092195 10月 3 11:43 hadoop-2.7.3.tar.gzdrwxr-xr-x. 2 root root 6 4月 11 12:59 include-rwxr-xr-x. 1 root root 170023183 10月 2 17:28 jdk-8u181-linux-x64.rpm-rw-r--r--. 1 root root 185646832 10月 2 16:31 jdk-8u181-linux-x64.tar.gzdrwxr-xr-x. 2 root root 6 4月 11 12:59 libdrwxr-xr-x. 2 root root 6 4月 11 12:59 lib64drwxr-xr-x. 2 root root 6 4月 11 12:59 libexecdrwxr-xr-x. 2 root root 6 4月 11 12:59 sbindrwxr-xr-x. 5 root root 49 10月 2 13:00 sharedrwxr-xr-x. 2 root root 6 4月 11 12:59 src 1234567891011121314151617[root@master100 local]# mv hadoop-2.7.3 hadoop[root@master100 local]# ll总用量 556412drwxr-xr-x. 2 root root 6 4月 11 12:59 bindrwxr-xr-x. 2 root root 6 4月 11 12:59 etcdrwxr-xr-x. 2 root root 6 4月 11 12:59 gamesdrwxr-xr-x. 9 root root 149 8月 18 2016 hadoop-rw-r--r--. 1 root root 214092195 10月 3 11:43 hadoop-2.7.3.tar.gzdrwxr-xr-x. 2 root root 6 4月 11 12:59 include-rwxr-xr-x. 1 root root 170023183 10月 2 17:28 jdk-8u181-linux-x64.rpm-rw-r--r--. 1 root root 185646832 10月 2 16:31 jdk-8u181-linux-x64.tar.gzdrwxr-xr-x. 2 root root 6 4月 11 12:59 libdrwxr-xr-x. 2 root root 6 4月 11 12:59 lib64drwxr-xr-x. 2 root root 6 4月 11 12:59 libexecdrwxr-xr-x. 2 root root 6 4月 11 12:59 sbindrwxr-xr-x. 5 root root 49 10月 2 13:00 sharedrwxr-xr-x. 2 root root 6 4月 11 12:59 src 8、将hadoop文件夹的主：组设置成hadoop1234567891011121314151617[root@master100 local]# chown -R hadoop:hadoop hadoop[root@master100 local]# ll总用量 556412drwxr-xr-x. 2 root root 6 4月 11 12:59 bindrwxr-xr-x. 2 root root 6 4月 11 12:59 etcdrwxr-xr-x. 2 root root 6 4月 11 12:59 gamesdrwxr-xr-x. 9 hadoop hadoop 149 8月 18 2016 hadoop-rw-r--r--. 1 root root 214092195 10月 3 11:43 hadoop-2.7.3.tar.gzdrwxr-xr-x. 2 root root 6 4月 11 12:59 include-rwxr-xr-x. 1 root root 170023183 10月 2 17:28 jdk-8u181-linux-x64.rpm-rw-r--r--. 1 root root 185646832 10月 2 16:31 jdk-8u181-linux-x64.tar.gzdrwxr-xr-x. 2 root root 6 4月 11 12:59 libdrwxr-xr-x. 2 root root 6 4月 11 12:59 lib64drwxr-xr-x. 2 root root 6 4月 11 12:59 libexecdrwxr-xr-x. 2 root root 6 4月 11 12:59 sbindrwxr-xr-x. 5 root root 49 10月 2 13:00 sharedrwxr-xr-x. 2 root root 6 4月 11 12:59 src 9、/usr目录与/usr/local目录所属主：组均为root，默认权限为755，也就是说其他用户（hadoop）没有写入（w）权限，在这里我们需要将这两个目录其他用户的权限设置为7，命令如下：12chmod 757 /usrchmod 757 /usr/local 可以看到执行情况如下：12345678910111213141516171819202122232425262728293031323334353637383940[root@master100 local]# cd /[root@master100 /]# chmod 757 /usr[root@master100 /]# ll总用量 16lrwxrwxrwx. 1 root root 7 10月 2 13:00 bin -&gt; usr/bindr-xr-xr-x. 5 root root 4096 10月 2 13:06 bootdrwxr-xr-x. 20 root root 3240 10月 3 11:47 devdrwxr-xr-x. 76 root root 8192 10月 3 11:47 etcdrwxr-xr-x. 3 root root 20 10月 3 10:58 homelrwxrwxrwx. 1 root root 7 10月 2 13:00 lib -&gt; usr/liblrwxrwxrwx. 1 root root 9 10月 2 13:00 lib64 -&gt; usr/lib64drwxr-xr-x. 2 root root 6 4月 11 12:59 mediadrwxr-xr-x. 2 root root 6 4月 11 12:59 mntdrwxr-xr-x. 2 root root 6 4月 11 12:59 optdr-xr-xr-x. 133 root root 0 10月 3 11:46 procdr-xr-x---. 3 root root 160 10月 3 11:50 rootdrwxr-xr-x. 24 root root 720 10月 3 11:47 runlrwxrwxrwx. 1 root root 8 10月 2 13:00 sbin -&gt; usr/sbindrwxr-xr-x. 2 root root 6 4月 11 12:59 srvdr-xr-xr-x. 13 root root 0 10月 3 11:47 sysdrwxrwxrwt. 10 root root 253 10月 3 11:47 tmpdrwxr-xrwx. 14 root root 167 10月 2 17:57 usrdrwxr-xr-x. 19 root root 267 10月 2 13:08 var[root@master100 /]# chmod 757 /usr/local[root@master100 /]# cd /usr[root@master100 usr]# ll总用量 104dr-xr-xr-x. 2 root root 24576 10月 2 17:57 bindrwxr-xr-x. 2 root root 6 4月 11 12:59 etcdrwxr-xr-x. 2 root root 6 4月 11 12:59 gamesdrwxr-xr-x. 3 root root 23 10月 2 13:01 includedrwxr-xr-x. 3 root root 61 10月 2 17:57 javadr-xr-xr-x. 27 root root 4096 10月 2 13:02 libdr-xr-xr-x. 37 root root 20480 10月 2 13:02 lib64drwxr-xr-x. 20 root root 4096 10月 2 13:02 libexecdrwxr-xrwx. 13 root root 237 10月 3 11:55 localdr-xr-xr-x. 2 root root 12288 10月 2 13:02 sbindrwxr-xr-x. 75 root root 4096 10月 2 13:02 sharedrwxr-xr-x. 4 root root 34 10月 2 13:00 srclrwxrwxrwx. 1 root root 10 10月 2 13:00 tmp -&gt; ../var/tmp Hadoop环境变量配置1、编辑/etc/profile文件1vi /etc/profile 2、在末尾加上如下几行12export HADOOP_HOME=/usr/local/hadoopexport PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH 这里注意HADOOP_HOME与PATH的顺序，我这里最后几行的配置如下：12345export JAVA_HOME=/usr/java/jdk1.8.0_181-amd64export HADOOP_HOME=/usr/local/hadoopexport PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JRE_HOME=$JAVA_HOME/jre 3、配置完环境变量之后保存退出，让环境变量立即生效1source /etc/profile 测试环境变量测试 输入如下命令：12hadoophadoop version 效果如下：123456789101112131415161718192021222324252627[root@master100 usr]# hadoopUsage: hadoop [--config confdir] [COMMAND | CLASSNAME] CLASSNAME run the class named CLASSNAME or where COMMAND is one of: fs run a generic filesystem user client version print the version jar &lt;jar&gt; run a jar file note: please use "yarn jar" to launch YARN applications, not this command. checknative [-a|-h] check native hadoop and compression libraries availability distcp &lt;srcurl&gt; &lt;desturl&gt; copy file or directories recursively archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive classpath prints the class path needed to get the credential interact with credential providers Hadoop jar and the required libraries daemonlog get/set the log level for each daemon trace view and modify Hadoop tracing settings Most commands print help when invoked w/o parameters.[root@master100 usr]# hadoop versionHadoop 2.7.3Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccffCompiled by root on 2016-08-18T01:41ZCompiled with protoc 2.5.0From source with checksum 2e4ce5f957ea4db193bce3734ff29ff4This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar 演示Hadoop自带的MapReduce例子 在这里用hadoop账户登录系统进行测试12345678910111213Xshell 6 (Build 0095)Copyright (c) 2002 NetSarang Computer, Inc. All rights reserved. Type `help' to learn how to use Xshell prompt.[C:\~]$ Connecting to 192.168.33.100:22...Connection established.To escape to local shell, press 'Ctrl+Alt+]'. WARNING! The remote SSH server rejected X11 forwarding request.Last login: Wed Oct 3 11:25:59 2018[hadoop@master100 ~]$ Hadoop 默认模式为非分布式模式，无需进行其他配置即可运行。非分布式即单 Java 进程，方便进行调试。 Hadoop 附带了丰富的例子（运行 ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar 可以看到所有例子），包括 wordcount、terasort、join、grep 等。在此我们选择运行 grep 例子，我们将 input 文件夹中的所有文件作为输入，筛选当中符合正则表达式 dfs[a-z.]+ 的单词并统计出现的次数，最后输出结果到 output 文件夹中。123456789101112131415161718192021222324252627282930313233343536[hadoop@master100 ~]$ cd /usr/local/hadoop [hadoop@master100 hadoop]$ ll总用量 108drwxr-xr-x. 2 hadoop hadoop 194 8月 18 2016 bindrwxr-xr-x. 3 hadoop hadoop 20 8月 18 2016 etcdrwxr-xr-x. 2 hadoop hadoop 106 8月 18 2016 includedrwxr-xr-x. 3 hadoop hadoop 20 8月 18 2016 libdrwxr-xr-x. 2 hadoop hadoop 239 8月 18 2016 libexec-rw-r--r--. 1 hadoop hadoop 84854 8月 18 2016 LICENSE.txt-rw-r--r--. 1 hadoop hadoop 14978 8月 18 2016 NOTICE.txt-rw-r--r--. 1 hadoop hadoop 1366 8月 18 2016 README.txtdrwxr-xr-x. 2 hadoop hadoop 4096 8月 18 2016 sbindrwxr-xr-x. 4 hadoop hadoop 31 8月 18 2016 share [hadoop@master100 hadoop]$ mkdir ./input # 将配置文件作为输入文件[hadoop@master100 hadoop]$ cp ./etc/hadoop/* ./input [hadoop@master100 hadoop]$ hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar grep ./input ./output 'dfs[a-z.]+' ........... #查看运行结果[hadoop@master100 hadoop]$ cat ./output/*6 dfs.audit.logger4 dfs.class3 dfs.server.namenode.2 dfs.period2 dfs.audit.log.maxfilesize2 dfs.audit.log.maxbackupindex1 dfsmetrics.log1 dfsadmin1 dfs.servers1 dfs.file 更多的MapReduce例子在这里就不一一测试了，有兴趣的可以自己去测试 相关推荐VMware虚拟机安装Linux系统 Linux系统下安装Java环境 CentOS 7部署Hadoop（伪分布式） CentOS 7部署Hadoop集群（完全分布式） CentOS 7部署Hadoop集群（HA高可用集群）]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Linux</tag>
        <tag>Hadoop配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统下安装Java环境]]></title>
    <url>%2Fpost%2FLinux%E7%B3%BB%E7%BB%9F%E4%B8%8B%E5%AE%89%E8%A3%85Java%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[测试环境Linux系统版本：CentOS 7 64位 终端模拟软件：Xshell 6 Java版本：jdk-8u181-linux-x64 下载JDK下载地址：1https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 进入页面后下载64位的包，jdk-8u181-linux-x64.tar.gz与jdk-8u181-linux-x64.rpm都可以，下面会分别介绍两种包的安装方法，选择其中一种进行安装即可。 终端模拟软件我这里使用的是Xshell，使用其他软件也可以，下载安装完成后用root用户连接Linux（SSH连接，默认端口：22），不会操作的自己百度，连接之后界面如下： 安装前准备检查系统是否已经有JDK，输入如下命令查看是否系统中是否已安装，部分人在安装CentOS 7时系统会自动安装JDK： 1java -version 如果系统没有安装，输入命令后提示如下（中文版和英文版在语言上会有些区别，但是提示的意思都一样），没有安装的可以直接跳过这里看下面的安装方法了： 12[root@master100 ~]# java -version-bash: java: 未找到命令 如果显示如下版本信息说明已经安装，可以直接使用系统的JDK，不需要自己安装了 1234[root@master100 ~]# java -versionjava version "1.8.0_181"Java(TM) SE Runtime Environment (build 1.8.0_181-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode) 如果想使用其他版本的JDK，需要先卸载后再安装，卸载方法可以参考下文各种包的卸载方法。 tar包的安装方法1、将下载到本地的文件上传到远程服务器的/usr/local目录下， 在Xshell上新建一个文件传输，如图： 在sftp文件传输命令框内输入如下命令： 1cd /usr/local 把下载好的文件拖到黑框里面就可以上传了，上传完成后有如下提示： 12Uploading jdk-8u181-linux-x64.tar.gz to remote:/usr/local/jdk-8u181-linux-x64.tar.gzsftp: sent 177 MB in 6.13 seconds 我们也可以打开目录查看一下文件，回到SSH窗口，命令如下： 12cd /usr/localll 我们可以看到自己上传的jdk-8u181-linux-x64.tar.gz 文件 1234567891011121314[root@master100 ~]# cd /usr/local[root@master100 local]# ll总用量 181296drwxr-xr-x. 2 root root 6 4月 11 12:59 bindrwxr-xr-x. 2 root root 6 4月 11 12:59 etcdrwxr-xr-x. 2 root root 6 4月 11 12:59 gamesdrwxr-xr-x. 2 root root 6 4月 11 12:59 include-rw-r--r--. 1 root root 185646832 10月 2 16:31 jdk-8u181-linux-x64.tar.gzdrwxr-xr-x. 2 root root 6 4月 11 12:59 libdrwxr-xr-x. 2 root root 6 4月 11 12:59 lib64drwxr-xr-x. 2 root root 6 4月 11 12:59 libexecdrwxr-xr-x. 2 root root 6 4月 11 12:59 sbindrwxr-xr-x. 5 root root 49 10月 2 13:00 sharedrwxr-xr-x. 2 root root 6 4月 11 12:59 src 2、解压文件，命令如下，输入后会出现一长串的提示： 1tar -zxvf /usr/local/jdk-8u181-linux-x64.tar.gz 3、解压后文件夹名为：jdk1.8.0_181 1234567891011121314[root@master100 local]# ll总用量 181296drwxr-xr-x. 2 root root 6 4月 11 12:59 bindrwxr-xr-x. 2 root root 6 4月 11 12:59 etcdrwxr-xr-x. 2 root root 6 4月 11 12:59 gamesdrwxr-xr-x. 2 root root 6 4月 11 12:59 includedrwxr-xr-x. 7 10 143 245 7月 7 16:09 jdk1.8.0_181-rw-r--r--. 1 root root 185646832 10月 2 16:31 jdk-8u181-linux-x64.tar.gzdrwxr-xr-x. 2 root root 6 4月 11 12:59 libdrwxr-xr-x. 2 root root 6 4月 11 12:59 lib64drwxr-xr-x. 2 root root 6 4月 11 12:59 libexecdrwxr-xr-x. 2 root root 6 4月 11 12:59 sbindrwxr-xr-x. 5 root root 49 10月 2 13:00 sharedrwxr-xr-x. 2 root root 6 4月 11 12:59 src 在这里修改一下文件名，将jdk1.8.0_181改成java，方便我们后面配置环境变量，在这也可以不修改，在配置环境变量时要注意文件名不能写错，修改文件名命令如下：1mv /usr/local/jdk1.8.0_181 /usr/local/java 4、修改配置文件，配置环境变量，在命令行输入：1vi /etc/profile 输入“G”定位到最后一行，按“i”进入编辑模式，在最下面添加如下几行信息：1234export JAVA_HOME=/usr/local/javaexport PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JRE_HOME=$JAVA_HOME/jre 如图： 添加完之后按ESC退出编辑模式，输入:wq后回车（保存并退出），如图： 5、让配置文件生效，可以输入如下命令或者是重启系统12345source /etc/profile重启命令：init 6reboot 6、查看安装情况1java -version 安装成功后会出现如下版本信息：1234[root@master100 ~]# java -versionjava version "1.8.0_181"Java(TM) SE Runtime Environment (build 1.8.0_181-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode) tar包的卸载将安装包删除，然后把配置文件内添加的环境变量删除 删除安装包的命令如下：1rm -rf /usr/local/java 修改配置文件的方法和上述配置环境变量一样 rpm包的安装方法1、将下载到本地的文件上传到远程服务器的/usr/local目录下， 在Xshell上新建一个文件传输，如图： 在sftp文件传输命令框内输入如下命令：1cd /usr/local 把下载好的文件拖到黑框里面就可以上传了，上传完成提示如下：123sftp:/usr/local&gt; Uploading jdk-8u181-linux-x64.rpm to remote:/usr/local/jdk-8u181-linux-x64.rpmsftp: sent 162 MB in 1.92 seconds 2、上传好之后回到命令框，开始安装我们的rpm包，首先我们要赋予安装包执行的权限，命令如下：1chmod 755 /usr/local/jdk-8u181-linux-x64.rpm 3、安装rpm包1rpm -ivh /usr/local/jdk-8u181-linux-x64.rpm 出现如下提示信息：1234567891011121314[root@master100 local]# rpm -ivh /usr/local/jdk-8u181-linux-x64.rpm警告：/usr/local/jdk-8u181-linux-x64.rpm: 头V3 RSA/SHA256 Signature, 密钥 ID ec551f03: NOKEY准备中... ################################# [100%]正在升级/安装... 1:jdk1.8-2000:1.8.0_181-fcs ################################# [100%]Unpacking JAR files... tools.jar... plugin.jar... javaws.jar... deploy.jar... rt.jar... jsse.jar... charsets.jar... localedata.jar... 4、配置环境变量 在命令行输入：1vi /etc/profile 输入“G”定位到最后一行，按“i”进入编辑模式，在最下面添加如下几行信息：1234export JAVA_HOME=/usr/java/jdk1.8.0_181-amd64export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JRE_HOME=$JAVA_HOME/jre 5、让配置文件生效，可以输入如下命令或者是重启系统12345source /etc/profile 重启命令：init 6reboot 6、查看安装情况1java -version 安装成功后会出现如下版本信息：1234[root@master100 jdk1.8.0_181-amd64]# java -versionjava version "1.8.0_181"Java(TM) SE Runtime Environment (build 1.8.0_181-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode) rpm包的卸载1、输入命令查看rpm包安装信息，如果第一条未出现信息，可以使用第二条命令查看，如果都没有可能是未安装或者是tar包安装的1rpm -qa | grep jdk 我用上述rpm包安装之后，用命令查看，提示如下：12[root@master100 local]# rpm -qa | grep jdkjdk1.8-1.8.0_181-fcs.x86_64 2、知道安装的rpm包名后就可以卸载了，卸载命令如下：1rpm -e --nodeps jdk1.8-1.8.0_181-fcs.x86_64 3、卸载完成后删除环境变量12vi /etc/profilesource /etc/profile 相关推荐VMware虚拟机安装Linux系统 CentOS 7部署Hadoop（单机版） CentOS 7部署Hadoop（伪分布式） CentOS 7部署Hadoop集群（完全分布式） CentOS 7部署Hadoop集群（HA高可用集群）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>JDK</tag>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMware虚拟机安装Linux系统]]></title>
    <url>%2Fpost%2FVMware%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AE%89%E8%A3%85Linux%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[测试环境操作系统：Windows 10, 64-bit 虚拟机：VMware® Workstation 14 Pro（版本号：14.0.0 build-6661328） Linux镜像版本：CentOS-7-x86_64-DVD-1804.iso VMware虚拟机安装1、按照软件提示界面安装软件，安装过程就不做说明了； 附上软件注册机：https://github.com/PengShuaixin/ToolsLibrary/tree/master/Vmware 2、打开软件—-编辑—-虚拟网络编辑器 获取管理员权限即可，如图： 3、在这里我们将虚拟机路由网关设置成192.168.33.1，按下图设置： 先将子网IP设置成192.168.33.0，子网掩码：255.255.255.0，然后设置“NAT设置” NET设置参数如下： 4、打开控制面板—-网络和 Internet—-网络连接，设置VMnet8的IPv4协议； CentOS安装1、新建一个虚拟机，选择自定义安装，根据提示根据自己的实际情况选择配置； 这里选择自己镜像文件的路径， 根据自己的喜好命名虚拟机、选择虚拟机文件路径，后面两步分配CPU资源和内存资源根据自己电脑实际情况分配, 注意这里的网络设置要选择NET， IO控制器按照系统推荐的就行， 我这里磁盘设置如下图： 2、配置完成后启动虚拟机，鼠标点进去按上下键选择，选择Install CentOS 7后回车（Alt+Shift键可以释放鼠标）；等待一会儿就进入安装界面，为了方便展示设置这里语言设置我就选择简体中文了， 3、接下来我们对以下三个选项进行设置； 软件选择我就选“最小安装”了，有兴趣的可以自己选择安装一个桌面，选择完成后点“完成” 安装位置没有特殊需求情况下不需要手动设置，点开设置之后直接点完成即可， 下面是网络设置，在这里也可以先跳过这一步，进入系统之后我们可以直接修改配置文件设置， IPv4按如下设置，将该虚拟机设置成静态IP，IP地址为192.168.33.100： IPv6设置成忽略，如下图： 设置完成之后保存设置，保存之后打开右上角网络连接，最后点左上角的完成， 在这里补充一下安装过程设置主机名的方法： 4、设置完成后就可以开始安装了； 在这儿我们还需要对Root密码进行设置， 设置完密码后点完成，在输入密码的过程中注意不要使用小键盘，若系统提示密码过于简单，点两次完成即可保存设置的密码，最后就等待进度条完成安装。 5、出现如下界面之后我们点击右下角“重启”按钮重启一下系统就完成安装了。 CentOS设置主机名设置编辑hostname文件，这只是其中一种方法，想了解更多方法可以自己百度 1vi /etc/hostname 按键盘上的字母“i”进入编辑模式，修改主机名： 修改完成之后按ESC，输入“:wq”保存并退出，然后重启系统，以下两条命令任意一条都可以重启系统：12rebootinit 6 这里再扩展一下关机命令：123init 0shutdown -h nowhalt 重启完重新登录就完成修改了，如图： 网络设置在上述安装过程中没有配置网络的可以在此对网络进行设置，这里代码比较长，可以用Xshell连接虚拟主机进行配置，复制粘贴代码比较方便，此处建议在安装过程中就提前设置好网络。 修改ifcfg-ens33配置文件1vi /etc/sysconfig/network-scripts/ifcfg-ens33 配置如下：12345678910111213141516171819TYPE="Ethernet"PROXY_METHOD="none"BROWSER_ONLY="no"BOOTPROTO="none"DEFROUTE="yes"IPV4_FAILURE_FATAL="no"IPV6INIT="no"IPV6_AUTOCONF="yes"IPV6_DEFROUTE="yes"IPV6_FAILURE_FATAL="no"IPV6_ADDR_GEN_MODE="stable-privacy"NAME="ens33"UUID="fa0d7aef-28a6-41d3-9a20-262748c5d4a5"DEVICE="ens33"ONBOOT="yes"IPADDR="192.168.33.100"PREFIX="24"GATEWAY="192.168.33.1"DNS1="192.168.33.1" 注释：1234IPADDR="192.168.33.100" //IP地址PREFIX="24"//子网掩码255.255.255.0，也可以写成：NETMASK="255.255.255.0"GATEWAY="192.168.33.1"//网关DNS1="192.168.33.1"//DNS服务器，还可以在后面加一个DNS2 修改完之后保存退出，运行如下命令可让网络配置立即生效：1service network restart 还有一个地方的文件也是DNS设置文件，在这里上面已经设置了DNS服务器就不需要修改了，可以查看了解一下1cat /etc/resolv.conf CentOS测试系统登录输入登录用户名:root,回车，然后输入密码，密码为安装时自己设置的密码。输入密码时不会有任何显示，输入完之后回车即可登录。注意：不要用小键盘输入密码！！ 网络测试查看本机IP1ip a 会显示出本机IP信息： 外网测试（测试过程中按Ctrl+c可以终止正在执行的进程）：1ping www.baidu.com 测试结果如下就证明可以访问外网：1234567891011121314[root@master100 network-scripts]# ping www.baidu.comPING www.a.shifen.com (220.181.111.188) 56(84) bytes of data.64 bytes from 220.181.111.188 (220.181.111.188): icmp_seq=1 ttl=128 time=10.2 ms64 bytes from 220.181.111.188 (220.181.111.188): icmp_seq=2 ttl=128 time=3.48 ms64 bytes from 220.181.111.188 (220.181.111.188): icmp_seq=3 ttl=128 time=4.93 ms64 bytes from 220.181.111.188 (220.181.111.188): icmp_seq=4 ttl=128 time=12.0 ms64 bytes from 220.181.111.188 (220.181.111.188): icmp_seq=5 ttl=128 time=3.91 ms64 bytes from 220.181.111.188 (220.181.111.188): icmp_seq=6 ttl=128 time=3.86 ms64 bytes from 220.181.111.188 (220.181.111.188): icmp_seq=7 ttl=128 time=3.81 ms64 bytes from 220.181.111.188 (220.181.111.188): icmp_seq=8 ttl=128 time=10.8 ms^C--- www.a.shifen.com ping statistics ---8 packets transmitted, 8 received, 0% packet loss, time 7017msrtt min/avg/max/mdev = 3.484/6.642/12.094/3.464 ms 内网测试： ping路由IP地址：1ping 192.168.33.1 结果如下证明网络配置没有问题：123456789101112[root@master100 network-scripts]# ping 192.168.33.1PING 192.168.33.1 (192.168.33.1) 56(84) bytes of data.64 bytes from 192.168.33.1: icmp_seq=1 ttl=128 time=0.260 ms64 bytes from 192.168.33.1: icmp_seq=2 ttl=128 time=0.131 ms64 bytes from 192.168.33.1: icmp_seq=3 ttl=128 time=0.197 ms64 bytes from 192.168.33.1: icmp_seq=4 ttl=128 time=0.115 ms64 bytes from 192.168.33.1: icmp_seq=5 ttl=128 time=0.195 ms64 bytes from 192.168.33.1: icmp_seq=6 ttl=128 time=0.198 ms^C--- 192.168.33.1 ping statistics ---6 packets transmitted, 6 received, 0% packet loss, time 5000msrtt min/avg/max/mdev = 0.115/0.182/0.260/0.050 ms 相关推荐Linux系统下安装Java环境 CentOS 7部署Hadoop（单机版） CentOS 7部署Hadoop（伪分布式） CentOS 7部署Hadoop集群（完全分布式） CentOS 7部署Hadoop集群（HA高可用集群）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>CentOS</tag>
        <tag>VMware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Eclipse中GitHub插件的简单使用]]></title>
    <url>%2Fpost%2FEclipse%E4%B8%ADGitHub%E6%8F%92%E4%BB%B6%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[测试环境操作系统：Windows 10软件版本：Eclipse Neon（4.6） Eclipse IDE for Java EE GitHub简介GitHub是用于版本控制和协作的代码托管平台。它可以让您和其他人在任何地方协同工作[1]。 关于Git和GitHub: Github是代码托管平台，是协作的工具;而Git是版本控制工具。Git不需要联网，在本机就可以使用，例如我经常用它来保存论文修改的中间状态文稿；Git也可以和其他的代码托管平台结合使用[2]。 若想具体了解Git的使用方法，可以自己去网上学习，在这里给大家推荐：Git教程 - 廖雪峰的官方网站 Eclipse中GitHub插件的简单使用1、如果还没有GitHub的账号，就先去GitHub注册一个账号，有账号的可以跳过这一步； 打开网页可看到该界面，按照提示一步步注册完成即可。也可以点击右上角Sign up进入如下界面进行注册，具体的注册步骤在此就不讨论了。 2、注册完成后点击Sign in登录账号； 输入注册好的用户名，密码登录账号，出现如下界面： 3、新建一个代码仓库； 点击Start a project，进入如下界面 填写好存储库相关信息后点Create repository即可创建成功 4、获取仓库链接； 5、打开Eclipse； 点右上角Open Perspective， 选择左边界面中间的选项， 在弹出的窗口里面填写相关信息， 点Next，出现下面界面， 继续点Next， 修改路径之后，点Finish，出现如下界面说明GitHub仓库连接成功， 6、创建完成后，会自动同步远程仓库中的内容，打开我们刚刚设置的路径就能看到仓库里面的文件已经同步到本地了； 7、回到java工程页面，开始上传我们的项目； 8、右键项目，选择Team—-Share Project； 9、下图的方法会将项目文件移动到Git的本地文件夹中， 工程文件也被移动到了我们选择的本地仓库路径下面，如图： 10、先将工程提交到本地仓库； 右键项目文件—-Team—-Commit… 选中需要提交的修改，右键—-Add to index，接着填写好Commit Message，选择右下角Commit提交修改到本地仓库（Commit and Push…可以直接提交本地并同步到远程仓库，为方便下面的演示，就先提交到本地） 完成后如图： 11、提交到远程仓库 右键项目文件—-Team—-Remote—-Push… 选择第一个选项， 选择一条分支，Add Spec， Next—-Finish提交， 出现下面界面说明提交成功，点OK可以退出界面 12、到这里我们=可以去自己的GitHub网页验证一下，有自己提交的项目说明成功了， 如果没能成功提交，可以重新尝试，第一次使用该工具不建议用自己的代码进行测试，最好新建一个项目使用熟练后再去提交自己的项目。 GitHub仓库还有着更多好用的功能，比如说分支提交，在此就不做介绍了，有兴趣的朋友可以自己去研究学习。最后，欢迎大家在评论区对本文的不足之处进行补充说明。 参考网站[1] GitHub官方教程：https://guides.github.com/activities/hello-world/[2]如何高效入门Github？:https://www.jianshu.com/p/13d356e76659 本文部分内容来自于网络，如有侵权，请联系作者删除！]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Eclipse</tag>
        <tag>Github</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统配置SSH免密登录(多主机互通)]]></title>
    <url>%2Fpost%2FLinux%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AESSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95-%E5%A4%9A%E4%B8%BB%E6%9C%BA%E4%BA%92%E9%80%9A%2F</url>
    <content type="text"><![CDATA[本文测试环境: Linux系统镜像:CentOS-7-x86_64-DVD-1804.iso虚拟机版本:VMware-workstation-full-12.1.0-3272444 操作成功后的效果:每台主机可以本机SSH免密登录,也可以与其他主机之间实现SSH免密登录,也就是每台主机都可以一对多SSH免密登录. 现用虚拟机搭建三台主机,IP分别是:192.168.33.201 master1192.168.33.202 master2192.168.33.203 master3 SSH免密登录的原理在这里就不做介绍了,有兴趣的可以自己去网上找资料了解一下,在这里就直接上方法了. PS:authorized_keys:存放远程免密登录的公钥,主要通过这个文件记录多台机器的公钥id_rsa : 生成的私钥文件id_rsa.pub ： 生成的公钥文件know_hosts : 已知的主机公钥清单 方法一: 先选择其中一台主机,在该主机上生成公钥和私钥,再将公钥和私钥上传到其他主机上,具体操作如下:在这里我就选择master1进行操作以下操作了:1.登录Linux系统,根据自己实际情况选择登录用户,执行下面代码生成公钥私钥对:1ssh-keygen -t rsa 会出现如下提示,一路回车就行123456789101112131415161718192021[root@master1 ~]# ssh-keygen -t rsaGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): //这里回车Enter passphrase (empty for no passphrase): //这里回车Enter same passphrase again: //这里回车Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:df:71:f6:3e:bb:bb:6c:38:91:f4:bc:70:a1:dd:86:a9 root@master1The key's randomart image is:+--[ RSA 2048]----+| || || || . . || S o Ooo|| . . Oo*o|| . ..=.o|| Eo.= || o*B|+-----------------+ 注意：在程序提示输入passphrase时直接输入回车，表示无证书密码。 2.生成秘钥的默认目录为:~/.ssh,该目录下会生成下面两个文件: id_rsa id_rsa.pub 2.实现本地免密登录,将id_rsa.pub中的内容拷贝到authorized_keys 1ssh-copy-id localhost ~/.ssh目录下会生成一个新的文件:authorized_keys 3.完成上述步骤后就可以本地SSH免密登录了,运行下面代码出现一行登录时间就代表本地SSH免密登录成功 1ssh localhost 下面是本地SSH免密登录成功的标志:12[root@master1 ~]$ ssh localhostLast login: Mon Aug 27 08:41:20 2018 from 192.168.33.2 4.如果本机能成功SSH免密登录,先退出SSH登录: 1exit 再执行以下代码将本机的~/.ssh文件夹复制到其他主机上: 1scp -r ~/.ssh/* 192.168.33.202:~/.ssh 1scp -r ~/.ssh/* 192.168.33.203:~/.ssh 提示输入密码时,输入远程主机密码回车即可 5.测试SSH免密登录,这里就不发测试了,大家自行测试 方法二: 将每台机器生成的id_rsa.pub追加添加到同一个authorized_keys内,然后再将该authorized_keys发送到其他远程主机上. 具体步骤如下:1.在master1,master2,master3上分别执行: 1ssh-keygen -t rsa 与”方法一”内所述一样,一路回车即可,生成秘钥的默认目录为~/.ssh2.接着制作包含master1,master2,master3中所有id_rsa.pub的authorized_keys文件:此处在master 1上生成authorized_keys文件,在master1上执行: 1ssh-copy-id -i 192.168.33.201 在master2上执行:1ssh-copy-id -i 192.168.33.201 在master3上执行:1ssh-copy-id -i 192.168.33.201 注意:此处代码中的”-i”千万不要忘记了!!! 3.通过scp将master1上生成的authorized_keys文件发送给其他主机:在master1上执行1scp -r ~/.ssh/authorized_keys 192.168.33.202:~/.ssh 1scp -r ~/.ssh/authorized_keys 192.168.33.203:~/.ssh 提示输入密码时,输入远程主机密码回车即可 5.测试SSH免密登录,可先测试本机免密登录,再测试远程主机远程登录本机登录可用:1ssh localhost 远程登录将localhost换成远程主机IP即可比如在master1上登录master2,就在master1上执行:1ssh 192.168.33.202 相关故障处理:部分人在配置完成后可能出现无法登录的情况,错误代码我不太记得了,欢迎各位读者在下面补充.造成故障的原因是之前配置过程中配置失败,然后重新对SSH免密登录进行配置,配置完成后无法正常登录,解决方法如下: 删除各主机下~/.ssh目录中的known_hosts文件:1rm -rf ~/.ssh/known_hosts]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>SSH</tag>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Windows中安装Hadoop常见错误解决方法]]></title>
    <url>%2Fpost%2F%E5%9C%A8Windows%E4%B8%AD%E5%AE%89%E8%A3%85Hadoop%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本文的是在Windows10系统中运行的情况,其他版本的系统需自己尝试以下解决办法; Hadoop版本:hadoop-2.7.3 Windows系统中Hadoop的具体配置方法在此就不赘述了,请自行查阅相关文档,下面直接开始上问题解决方法: 错误一:12Error: JAVA_HOME is incorrectly set. Please update D:\hadoop\hadoop-2.7.3\conf\hadoop-env.cmd 该错误代码是Hadoop中java环境变量配置错误导致的,修改Hadoop配置文件即可解决 我的Hadoop安装目录在: D:\hadoop\hadoop-2.7.3,下面就以此为例, 配置文件存放在hadoop中的etc目录下,我们打开D:\hadoop\hadoop-2.7.3\etc\hadoop\hadoop-env.cmd文件,找到如下代码: 1set JAVA_HOME=%JAVA_HOME% 将%JAVA_HOME%用具体的jdk安装路径替代,比如说Java的安装路径为C:\Program Files\Java\,jdk版本为:jdk1.8.0_181则将上面的代码替换成如下代码:1set JAVA_HOME=C:\PROGRA~1\Java\jdk1.8.0_181 PS:此处的路径C:\Program Files中带有空格,直接写该路径系统无法正常识别,故在此用软链代替,将C:\Program Files写成C:\PROGRA~1;此处也可以用引号包围,即将C:\Program Files写成C:”\Program Files” 错误二: 启动hadoop进程时,namenode,nodemanager,datanode进程报如下错误代码:1234567891011121314java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method) at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:609) at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:977) at org.apache.hadoop.util.DiskChecker.checkAccessByFileMethods(DiskChecker.java:187) at org.apache.hadoop.util.DiskChecker.checkDirAccess(DiskChecker.java:174) at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:157) at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:2345) at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2387) at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2369) at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2261) at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2308) at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2485) at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2509) 该错误是电脑java版本导致的错误,部分电脑安装的是32位java环境.本机使用如下版本java(32位)测试时,会报上面所述错误代码,123java version "1.8.0_161"Java(TM) SE Runtime Environment (build 1.8.0_161-b12)Java HotSpot(TM) Client VM (build 25.161-b12, mixed mode) 具体解决方案如下:1.先用控制面板卸载计算机上的java;2.重新下载安装一个新的64位java;本文使用的是:jdk-8u181-windows-x64.exe版本信息如下:123java version "1.8.0_181"Java(TM) SE Runtime Environment (build 1.8.0_181-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode) 3.删除hadoop生成的namenode,datanode数据文件(文件路径在自己的hdfs-site.xml配置文件内有);比如hdfs-site.xml配置如下:1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/D:/hadoopdata/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/D:/hadoopdata/datanode&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 那么就删除D:\hadoopdata文件夹4.重新格式化HDFS1hdfs namenode -format 5.再次启动hadoop进程即可成功启动. PS:一般情况3,4步省略也可以成功启动hadoop的4个进程,如果有报错就操作一下3,4步,下面再添加一种数据文件导致错误的解决方法 错误三: hdaoop启动时datanode进程开启失败,系统错误代码如下:123456789101112131415161718192021java.io.IOException: Incompatible clusterIDs in D:\hadoopdata\datanode: namenode clusterID = CID-6f4dc54c-ee55-465a-b484-7a917fa1af74; datanode clusterID = CID-15453d83-8724-4e54-a770-f3a96712f52d at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:775) at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:300) at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:416) at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:395) at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:573) at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1362) at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1327) at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:317) at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223) at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:802) at java.lang.Thread.run(Thread.java:748)18/08/26 13:04:39 FATAL datanode.DataNode: Initialization failed for Block pool &lt;registering&gt; (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting.java.io.IOException: All specified directories are failed to load. at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:574) at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1362) at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1327) at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:317) at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223) at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:802) at java.lang.Thread.run(Thread.java:748) 首先,我们还是先讨论一下出现该错误的原因吧,我们在安装hadoop的过程中多少会有些问题,也常常会多次格式化HDFS文件系统,也正是因为这种多次格式化的操作才容易导致此错误,不仅仅是在Windows系统,在Linux系统中也很常见.为什么会出现这种情况呢?可以回想一下,我们在操作HDFS格式化时,并没有将datanode的数据文件删掉.从而因为namenode clusterID和datanode clusterID不同而datanode进程无法开启,那要怎么解决呢? 解决方案如下:1.停掉hadoop所有进程;2.删除datanode文件夹,datanode文件夹具体路径在上述”错误二”中有提到过,在此就不赘述了3.重新启动hadoop即可解决问题 Hadoop安装方法 在Windows中安装Hadoop（非虚拟机安装）]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
</search>
